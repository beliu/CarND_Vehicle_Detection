{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "### Import the necessary libraries and packages\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n",
    "import glob\n",
    "import time\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.ndimage.measurements import label\n",
    "from skimage.feature import hog\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import scipy.misc\n",
    "from collections import defaultdict\n",
    "import PyQt5\n",
    "%matplotlib qt5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define a function to extract the target objects from the larger Udacity image and store them to a folder\n",
    "def extract_images(data, filepath):\n",
    "    for ii, img in enumerate(data):\n",
    "         # Read in each one by one\n",
    "        file = img[0]\n",
    "        image = mpimg.imread(file)\n",
    "        # Isolate the region in the image that contains the object\n",
    "        xmin, ymin, xmax, ymax = img[1], img[2], img[3], img[4]\n",
    "        image = image[ymin:ymax + 1, xmin:xmax + 1]\n",
    "        image = cv2.resize(image, (64, 64))\n",
    "        filler = '00000'\n",
    "        mpimg.imsave(filepath + 'image_' + filler[:len(filler) - len(str(ii))] + str(ii) + '.jpg', image)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convert_color(img, conv=None):\n",
    "    image = np.copy(img)\n",
    "    if conv != 'RGB':\n",
    "        transform = \"cv2.cvtColor(image, cv2.COLOR_\" + conv + \")\"\n",
    "        features = eval(transform)\n",
    "    else:\n",
    "        features = image\n",
    "    \n",
    "    return features\n",
    "    \n",
    "# Define a function to compute binned color features  \n",
    "def bin_spatial(img, size=(32, 32), channel='all'):\n",
    "    if channel == 'all':\n",
    "        channel = np.arange(3)\n",
    "    \n",
    "    features = []\n",
    "    for ch in channel:\n",
    "        color = cv2.resize(img[:, :, ch], size).ravel()\n",
    "        features.append(color)\n",
    "    spatial_features = np.concatenate(features)\n",
    "            \n",
    "    return spatial_features\n",
    "\n",
    "# Define a function to compute color histogram features  \n",
    "def color_hist(img, nbins=32, bins_range=(0, 256), channel='all'):   \n",
    "    # Compute the histogram of the color channels separately\n",
    "    if channel == 'all':\n",
    "        channel = np.arange(3)\n",
    "        \n",
    "    features = []\n",
    "    hist_features = []\n",
    "    for ch in channel:\n",
    "        channel_hist = np.histogram(img[:, :, ch], bins=nbins, range=bins_range)\n",
    "        features.append(channel_hist[0])\n",
    "    hist_features = np.concatenate(features)\n",
    "        \n",
    "    # Return the individual histograms, bin_centers and feature vector\n",
    "    return hist_features\n",
    "\n",
    "# Define a function to return HOG features and visualization\n",
    "def get_hog_features(img, orient, pix_per_cell, cell_per_block, \n",
    "                        vis=False, feature_vec=True, trans_sqrt=True, block_norm='L1'):\n",
    "    # Call with two outputs if vis==True\n",
    "    if vis == True:\n",
    "        features, hog_image = hog(img, orientations=orient, pixels_per_cell=(pix_per_cell, pix_per_cell),\n",
    "                                  cells_per_block=(cell_per_block, cell_per_block), block_norm=block_norm,\n",
    "                                  transform_sqrt=trans_sqrt, visualise=vis, feature_vector=feature_vec)\n",
    "        return features, hog_image\n",
    "    # Otherwise call with one output\n",
    "    else:      \n",
    "        features = hog(img, orientations=orient, pixels_per_cell=(pix_per_cell, pix_per_cell),\n",
    "                       cells_per_block=(cell_per_block, cell_per_block), block_norm=block_norm,\n",
    "                       transform_sqrt=trans_sqrt, visualise=vis, feature_vector=feature_vec)\n",
    "        return features\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define a function to extract features from a list of images\n",
    "# Have this function call bin_spatial() and color_hist()\n",
    "def extract_features(data, img_format='png', spatial_dict=None, hist_dict=None, hog_dict=None):\n",
    "    \n",
    "    # Create a list to append feature vectors to\n",
    "    features = []\n",
    "    \n",
    "    # Iterate through the list of images\n",
    "    for file in data:\n",
    "        single_img_features = []\n",
    "        # Read in each one by one\n",
    "        image = mpimg.imread(file) \n",
    "        if ((img_format == 'jpg') | (file[-4:] == '.jpg')):\n",
    "            image = image.astype(np.float32) / 255\n",
    "        image = cv2.resize(image, (64, 64))\n",
    "        \n",
    "        if spatial_dict is not None:\n",
    "            cspace = spatial_dict['conv']\n",
    "            spatial_size = spatial_dict['size']\n",
    "            channels = spatial_dict['channels']\n",
    "            image_conv = convert_color(image, cspace)\n",
    "            spatial_features = bin_spatial(image_conv, size=(spatial_size, spatial_size), channel=channels)\n",
    "            single_img_features.append(spatial_features)\n",
    "            \n",
    "        if hist_dict is not None:\n",
    "            cspace = hist_dict['conv']\n",
    "            hist_bins = hist_dict['nbins']\n",
    "            bin_range = hist_dict['bin_range']\n",
    "            channels = hist_dict['channels']\n",
    "            image_conv = convert_color(image, cspace)\n",
    "            hist_features = color_hist(image_conv, nbins=hist_bins, bins_range=(0, 256), channel=channels)\n",
    "            single_img_features.append(hist_features)\n",
    "\n",
    "        if hog_dict is not None:\n",
    "            cspace = hog_dict['conv']\n",
    "            orient = hog_dict['orient']\n",
    "            pix_per_cell = hog_dict['pix_per_cell']\n",
    "            cell_per_block = hog_dict['cell_per_block']\n",
    "            channels = hog_dict['channels']       \n",
    "            trans_sqrt = hog_dict['trans_sqrt']\n",
    "            block_norm = hog_dict['block_norm']\n",
    "            image_conv = convert_color(image, cspace)  \n",
    "            hog_features = []\n",
    "            if channels == 'all':\n",
    "                channels = np.arange(3)\n",
    "            \n",
    "            for ch in channels:\n",
    "                hog_features.append(get_hog_features(image_conv[:,:,ch], \n",
    "                                    orient, pix_per_cell, cell_per_block, \n",
    "                                    vis=False, feature_vec=True, trans_sqrt=trans_sqrt, block_norm=block_norm))\n",
    "            hog_features = np.ravel(hog_features)                      \n",
    "            single_img_features.append(hog_features)\n",
    "            \n",
    "        features.append(np.concatenate(single_img_features))\n",
    "\n",
    "    # Return list of feature vectors\n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Save a dictionary into a pickle file\n",
    "def save_to_pickle(data, key_name, file_name):\n",
    "    if len(file_name) > 1:\n",
    "        for d, k, f in zip(data, key_name, file_name):\n",
    "            pickle_data = {k: d}\n",
    "            pickle.dump(pickle_data, open(f + '.p', \"wb\"))\n",
    "    else:\n",
    "        pickle_data = {}\n",
    "        for d, k in zip(data, key_name):\n",
    "            pickle_data[k] = d\n",
    "        print(pickle_data)\n",
    "        pickle.dump(pickle_data, open(file_name[0] + '.p', \"wb\"))\n",
    "    \n",
    "    return None\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_training_files(GTI=False, KITTI=False, udacity=False, len_non_cars=None):\n",
    "    \n",
    "    car_gti = []\n",
    "    kitti_cars = []\n",
    "    car_gti_far = []\n",
    "    car_gti_left = []\n",
    "    car_gti_midclose = []\n",
    "    car_gti_right = []\n",
    "    \n",
    "    if GTI:\n",
    "        # Collect the filenames for all car images in the dataset\n",
    "        car_gti_far = glob.glob('../data/vehicles/GTI_Far/*png')\n",
    "        car_gti_left = glob.glob('../data/vehicles/GTI_Left/*png')\n",
    "        car_gti_midclose = glob.glob('../data/vehicles/GTI_MiddleClose/*png')\n",
    "        car_gti_right = glob.glob('../data/vehicles/GTI_Right/*png')\n",
    "        car_gti = car_gti_far + car_gti_left + car_gti_midclose + car_gti_right\n",
    "    if KITTI:\n",
    "        kitti_cars = glob.glob('../data/vehicles/KITTI_extracted/*png')\n",
    "\n",
    "    # Collect the filenames for all non-car images in the dataset\n",
    "    non_car_gti = glob.glob('../data/non_vehicles/GTI/*png')\n",
    "    extras_non_cars = glob.glob('../data/non_vehicles/Extras/*png')\n",
    "    \n",
    "    car_files = np.array(car_gti + kitti_cars)\n",
    "    non_car_files = np.array(non_car_gti + extras_non_cars)\n",
    "    non_car_files = shuffle(non_car_files)\n",
    "    if len_non_cars: \n",
    "        n_sample = len_non_cars\n",
    "    else:\n",
    "        n_sample = len(non_car_files)\n",
    "    \n",
    "    udacity_car_files = np.array([])\n",
    "    if udacity:\n",
    "        udacity_df = pd.read_csv('../data/labels.csv')\n",
    "        udacity_cars_df = udacity_df.loc[(udacity_df.label == 'car') & (udacity_df.occluded == 0)]\n",
    "\n",
    "        udacity_car_idx = udacity_cars_df.index.tolist()\n",
    "        udacity_car_files = np.array(glob.glob('../data/udacity_cropped/*jpg'))\n",
    "        udacity_car_files = udacity_car_files[udacity_car_idx]\n",
    "\n",
    "    all_img_files = np.hstack((non_car_files[:n_sample], car_files, udacity_car_files))\n",
    "    img_type_labels = np.hstack((np.zeros(n_sample), np.ones(len(car_gti_far)), np.ones(len(car_gti_left)) * 2, \n",
    "                                     np.ones(len(car_gti_midclose)) * 3, np.ones(len(car_gti_right)) * 4, \n",
    "                                     np.ones(len(kitti_cars)) * 5, np.ones(len(udacity_car_files)) * 6))\n",
    "\n",
    "    print('No. of GTI car files: {}'.format(len(car_gti)))\n",
    "    print('No. of KITTI car files: {}'.format(len(kitti_cars)))\n",
    "    print('No. of Udacity car files: {}'.format(len(udacity_car_files)))\n",
    "    print('Total number of car files: {}'.format(len(car_files) + len(udacity_car_files)))\n",
    "    print('Tota number of non-car files: {}'.format(n_sample))\n",
    "    print('Total number of images: {}'.format(all_img_files.shape))\n",
    "    print('Total number of labels: {}'.format(img_type_labels.shape))\n",
    "\n",
    "    train_img_files = np.copy(all_img_files)\n",
    "    train_labels_copy = np.copy(img_type_labels)\n",
    "    \n",
    "    return train_img_files, train_labels_copy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# train_img_files, train_labels_copy = make_training_files(GTI=True, KITTI=True, udacity=True)\n",
    "\n",
    "# img_format = 'png'\n",
    "# ## Here is a list of possible color spaces to use\n",
    "# '''cv2.COLOR_RGB2HSV\n",
    "#    cv2.COLOR_RGB2LUV\n",
    "#    cv2.COLOR_RGB2HLS\n",
    "#    cv2.COLOR_RGB2YUV\n",
    "#    cv2.COLOR_RGB2YCrCb'''\n",
    "\n",
    "# spatial_dict = {'conv': 'RGB2YCrCb', 'size': 32, 'channels': 'all'}\n",
    "# hist_dict = {'conv': 'RGB2YCrCb', 'nbins': 32, 'bin_range': (0, 256), 'channels': 'all'}\n",
    "# hog_dict = {'conv': 'RGB2YCrCb', 'orient': 9, 'pix_per_cell': 8, 'cell_per_block': 2, 'channels': 'all',\n",
    "#             'trans_sqrt': True, 'block_norm': 'L1'}\n",
    "\n",
    "# # X_train_features = extract_features(train_img_files, spatial_dict=spatial_dict, hist_dict=hist_dict, \n",
    "# #                                     hog_dict=hog_dict)\n",
    "# # file_end = 'ycrcb_massive_data'\n",
    "\n",
    "# # X_train_features = np.array(X_train_features).astype(np.float32)\n",
    "# # # Fit a per-column scaler\n",
    "# # X_scaler = StandardScaler().fit(X_train_features)\n",
    "# # # Apply the scaler to X\n",
    "# # scaled_X = X_scaler.transform(X_train_features)\n",
    "# print('X train features shape is {}'.format(X_train_features.shape))\n",
    "# print('Scaled X features is {}'.format(scaled_X.shape))\n",
    "\n",
    "# save_to_pickle([scaled_X, train_labels_copy, X_scaler], ['features', 'labels', 'scaler'], \n",
    "#                ['../pickle/scaled_X_' + file_end])\n",
    "# save_to_pickle([spatial_dict, hist_dict, hog_dict], ['spatial', 'hist', 'hog'], \n",
    "#                ['../pickle/dict_' + file_end])\n",
    "\n",
    "# rand_state = np.random.randint(0, 100)\n",
    "# # Shuffle and split the data into a training and test set\n",
    "# X_train, X_validate, y_train, y_validate = train_test_split(scaled_X, train_labels_copy, test_size=0.2, \n",
    "#                                                             stratify=train_labels_copy, random_state=rand_state)\n",
    "# print('X_train shape is {}'.format(X_train.shape))\n",
    "# print('y_train shape is {}'.format(y_train.shape))\n",
    "# print('X_validate shape is {}'.format(X_validate.shape))\n",
    "# print('y_validate shape is {}'.format(y_validate.shape))\n",
    "\n",
    "# print(y_train[:20])\n",
    "# y_train[y_train != 0] = 1.\n",
    "# y_validate[y_validate != 0] = 1.\n",
    "# print(y_train[:20])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Use a linear SVC \n",
    "# svc = LinearSVC()\n",
    "# # Check the training time for the SVC\n",
    "# t=time.time()\n",
    "# svc.fit(X_train, y_train)\n",
    "# t2 = time.time()\n",
    "# print(round(t2-t, 2), 'Seconds to train SVC...')\n",
    "# print('Test Accuracy of SVC = ', round(svc.score(X_validate, y_validate), 4))\n",
    "# save_to_pickle([svc, hog_dict['trans_sqrt'], hog_dict['block_norm']], ['svc', 'trans_sqrt', 'block_norm'], \n",
    "#                ['../pickle/svc_pickle_' + file_end])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file_end = 'ycrcb'\n",
    "dict_pickle = pickle.load( open(\"../pickle/scaled_X_\" + file_end + \".p\", \"rb\") )\n",
    "X_scaler = dict_pickle['scaler']\n",
    "\n",
    "dict_pickle = pickle.load( open(\"../pickle/dict_\" + file_end + \".p\", \"rb\") )\n",
    "spatial_dict = dict_pickle['spatial']\n",
    "hist_dict = dict_pickle['hist']\n",
    "hog_dict = dict_pickle['hog']\n",
    "\n",
    "dict_pickle = pickle.load( open(\"../pickle/svc_pickle_\" + file_end + \".p\", \"rb\" ) )\n",
    "svc = dict_pickle['svc']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define a single function that can extract features using hog sub-sampling \n",
    "# and make predictions\n",
    "def find_cars(img, img_format, xstart, xstop, ystart, ystop, scale, X_scaler, svc, hog_dict, \n",
    "              spatial_dict=None, hist_dict=None, print_pred=False):\n",
    "                \n",
    "    # Extract the Hog parameters\n",
    "    hg_cspace = hog_dict['conv']\n",
    "    orient = hog_dict['orient']\n",
    "    pix_per_cell = hog_dict['pix_per_cell']\n",
    "    cell_per_block = hog_dict['cell_per_block']\n",
    "    hg_ch = hog_dict['channels']       \n",
    "    trans_sqrt = hog_dict['trans_sqrt']\n",
    "    block_norm = hog_dict['block_norm']\n",
    "    \n",
    "    if hg_ch == 'all':\n",
    "        hg_ch = np.arange(3)\n",
    "    \n",
    "    draw_img = np.copy(img)\n",
    "    if img_format == 'jpg':\n",
    "        img = img.astype(np.float32) / 255\n",
    "     \n",
    "    # Compute individual channel HOG features for the entire image\n",
    "    img_tosearch = img[ystart:ystop, xstart:xstop, :]\n",
    "    ctrans_tosearch = convert_color(img_tosearch, conv=hg_cspace)\n",
    "    bboxes = [] # Store the coordinates of the bounded boxes\n",
    "\n",
    "    if scale != 1:\n",
    "        imshape = ctrans_tosearch.shape\n",
    "        ctrans_tosearch = cv2.resize(ctrans_tosearch, (np.int(imshape[1] / scale), \n",
    "                                     np.int(imshape[0] / scale)))\n",
    "    chs = []\n",
    "    for ch in hg_ch:\n",
    "        chs.append(ctrans_tosearch[:,:,ch])\n",
    "\n",
    "    # Define blocks and steps\n",
    "    nxblocks = (chs[0].shape[1] // pix_per_cell) - cell_per_block + 1\n",
    "    nyblocks = (chs[0].shape[0] // pix_per_cell) - cell_per_block + 1 \n",
    "    nfeat_per_block = orient*cell_per_block**2\n",
    "\n",
    "    # 64 was the orginal sampling rate, with 8 cells and 8 pix per cell\n",
    "    window = 64\n",
    "    nblocks_per_window = (window // pix_per_cell) - cell_per_block + 1\n",
    "    cells_per_step = 2  # Instead of overlap, define how many cells to step\n",
    "    nxsteps = (nxblocks - nblocks_per_window) // cells_per_step + 1\n",
    "    nysteps = (nyblocks - nblocks_per_window) // cells_per_step + 1\n",
    "\n",
    "    hogs = []\n",
    "    for ch in chs:\n",
    "        hog_feature = get_hog_features(ch, orient, pix_per_cell, cell_per_block, \n",
    "                                feature_vec=False, trans_sqrt=trans_sqrt, block_norm=block_norm)\n",
    "        hogs.append(hog_feature)\n",
    "    \n",
    "    for xb in range(nxsteps):\n",
    "        for yb in range(nysteps):\n",
    "            ypos = yb*cells_per_step\n",
    "            xpos = xb*cells_per_step\n",
    "\n",
    "            # Extract HOG for this patch\n",
    "            hog_list = []\n",
    "            for hg in hogs:    \n",
    "                hog_vect = hg[ypos:ypos+nblocks_per_window, \n",
    "                                 xpos:xpos+nblocks_per_window].ravel()\n",
    "                hog_list.append(hog_vect)   \n",
    "            hog_features = np.hstack(hog_list)\n",
    "        \n",
    "            xleft = xpos*pix_per_cell\n",
    "            ytop = ypos*pix_per_cell\n",
    "\n",
    "           # Extract the image patch\n",
    "            subimg = cv2.resize(ctrans_tosearch[ytop:ytop+window, \n",
    "                                xleft:xleft+window], (64,64))\n",
    "\n",
    "            # Get color features\n",
    "            spatial_features = np.array([])\n",
    "            hist_features = np.array([])\n",
    "            if spatial_dict is not None:\n",
    "                sp_cspace = spatial_dict['conv']\n",
    "                spatial_size = spatial_dict['size']\n",
    "                sp_ch = spatial_dict['channels']\n",
    "                spatial_features = bin_spatial(subimg, size=(spatial_size, spatial_size), channel=sp_ch)\n",
    "            if hist_dict is not None:\n",
    "                hs_cspace = hist_dict['conv']\n",
    "                hist_bins = hist_dict['nbins']\n",
    "                bin_range = hist_dict['bin_range']\n",
    "                hi_ch = hist_dict['channels']\n",
    "                hist_features = color_hist(subimg, nbins=hist_bins, bins_range=(0, 256), channel=hi_ch)\n",
    "\n",
    "            # Scale features and make a prediction\n",
    "            features = np.hstack((spatial_features, hist_features, hog_features))\n",
    "            test_features = X_scaler.transform(features.reshape(1, -1))       \n",
    "            test_prediction = svc.predict(test_features)\n",
    "            if print_pred:\n",
    "                print('prediction: {}'.format(test_prediction))\n",
    "            if test_prediction == 1:\n",
    "                xbox_left = np.int(xleft*scale)\n",
    "                ytop_draw = np.int(ytop*scale)\n",
    "                win_draw = np.int(window*scale)\n",
    "                cv2.rectangle(draw_img,(xbox_left+xstart, ytop_draw+ystart),\n",
    "                (xbox_left+xstart+win_draw,ytop_draw+win_draw+ystart),(0,0,255),6) \n",
    "                bboxes.append(((xbox_left + xstart, ytop_draw + ystart), \n",
    "                               (xbox_left + xstart + win_draw, ytop_draw + win_draw + ystart)))\n",
    "                \n",
    "    return draw_img, bboxes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_heat(heatmap, bbox_list):\n",
    "    # Iterate through list of bboxes\n",
    "    for box in bbox_list:\n",
    "        # Add += 1 for all pixels inside each bbox\n",
    "        # Assuming each \"box\" takes the form ((x1, y1), (x2, y2))\n",
    "        heatmap[box[0][1]:box[1][1], box[0][0]:box[1][0]] += 1\n",
    "\n",
    "    # Return updated heatmap\n",
    "    return heatmap\n",
    "\n",
    "def apply_threshold(heatmap, threshold):\n",
    "    # Zero out pixels below the threshold\n",
    "    output_heatmap = np.copy(heatmap)\n",
    "    output_heatmap[output_heatmap < threshold] = 0\n",
    "    \n",
    "    # Return thresholded map\n",
    "    return output_heatmap\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_labeled_bboxes(image, heat, threshold, draw, color=(0, 0, 255), thickness=3):\n",
    "    out_bboxes = {} # A dict of all the bounded boxes for each car found in this image\n",
    "    \n",
    "    # Accumulate bounded boxes across several frames and then apply heat-map with threshold\n",
    "    heat_map = apply_threshold(heat, threshold)\n",
    "    labels = label(heat_map)\n",
    "\n",
    "    # Iterate through all detected cars\n",
    "    for car_number in range(1, labels[1]+1):\n",
    "        # Find pixels with each car_number label value\n",
    "        nonzero = (labels[0] == car_number).nonzero()\n",
    "        # Identify x and y values of those pixels\n",
    "        nonzeroy = np.array(nonzero[0])\n",
    "        nonzerox = np.array(nonzero[1])\n",
    "        # Define a bounding box based on min/max x and y\n",
    "        bbox = ((np.min(nonzerox), np.min(nonzeroy)), (np.max(nonzerox), np.max(nonzeroy)))\n",
    "        out_bboxes[car_number] = bbox\n",
    "        if draw:\n",
    "            # Draw the box on the image\n",
    "            cv2.rectangle(image, bbox[0], bbox[1], color, thickness)\n",
    "    \n",
    "    if draw:\n",
    "        # Return the image and bboxes\n",
    "        return image, out_bboxes\n",
    "    else:\n",
    "        # Return the bboxes\n",
    "        return out_bboxes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from moviepy.editor import VideoFileClip\n",
    "from IPython.display import HTML\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define a class to keep track of data among frames\n",
    "class Frame():\n",
    "    def __init__(self):\n",
    "        self.image = None\n",
    "        self.frame_count = 0\n",
    "        self.car_accum = False\n",
    "        self.find_cars = True\n",
    "        self.show_bboxes = False\n",
    "        self.debug = False\n",
    "        self.detect_count = 0 # Tracks the number of frames that have passed\n",
    "        self.detect_samples = 10 # How many frames should we collect boxes for\n",
    "        self.detect_thresh = 9\n",
    "        self.track_count = 0\n",
    "        self.track_samples = 2        \n",
    "        self.track_thresh = 2\n",
    "        self.lost_count = 0\n",
    "        self.lost_thresh = 2\n",
    "        self.heat = None\n",
    "        self.heat_record = []\n",
    "        self.car_boxes = defaultdict(list)\n",
    "        self.final_boxes = defaultdict(list)\n",
    "\n",
    "    def show_bbox_details(self, thickness, colors, box_lists):\n",
    "        for t, color, box_list in zip(thickness, colors, box_lists):\n",
    "            for box in box_list:\n",
    "                cv2.rectangle(self.image, box[0], box[1], color, t)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1, '"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = '1'\n",
    "text += ', '\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This is the function that processes each frame of the video to find cars and apply a box around them\n",
    "def process_image(image):\n",
    "    \n",
    "    frame.image = np.copy(image)\n",
    "    if frame.heat is None:\n",
    "        frame.heat = np.zeros_like(frame.image[:, :, 0])\n",
    "    if frame.debug:\n",
    "        font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "        cv2.putText(frame.image, 'detect count: {}'.format(str(frame.detect_count)), (650, 150), font, 1, \n",
    "                    (200,255,155), 2, cv2.LINE_AA)\n",
    "        cv2.putText(frame.image, 'frame count: {}'.format(frame.frame_count), (650, 200), font, 1, \n",
    "                    (200,255,155), 2, cv2.LINE_AA)\n",
    "    \n",
    "    track_boxes = []\n",
    "    test_boxes = []\n",
    "    detect_boxes = []\n",
    "    accum_detect_boxes = []\n",
    "    search_box = []\n",
    "    car_boxes = []\n",
    "    \n",
    "    if frame.find_cars:\n",
    "        # Search for cars in the distance\n",
    "        xstart = 0\n",
    "        xstop = image.shape[1]\n",
    "        ycoords = [(400, 450), (400, 550), (400, 650)]\n",
    "#         scales = [0.5, 1, 1.5]\n",
    "        scales = [0.8, 1.1, 1.6] \n",
    "        for ycoord, scale in zip(ycoords, scales):\n",
    "            ystart, ystop = ycoord[0], ycoord[1]\n",
    "            _, bboxes = find_cars(image, img_format, xstart, xstop, ystart, ystop, scale, X_scaler,\n",
    "                                  svc, hog_dict, spatial_dict, hist_dict)\n",
    "            detect_boxes += bboxes\n",
    "        \n",
    "        if frame.car_accum:\n",
    "            # Accumulate bounded boxes across several frames and then apply heat-map with threshold\n",
    "            heat = np.zeros_like(frame.image[:, :, 0])\n",
    "            heat = add_heat(heat, detect_boxes)\n",
    "            car_boxes = make_labeled_bboxes(frame.image, heat, threshold=1, draw=False)\n",
    "            for car, box in car_boxes.items():\n",
    "                accum_detect_boxes += [box]\n",
    "            frame.heat = add_heat(frame.heat, accum_detect_boxes)\n",
    "        else:\n",
    "            frame.heat = add_heat(frame.heat, detect_boxes)\n",
    "        \n",
    "        frame.heat_record.append(np.copy(frame.heat))\n",
    "        if frame.detect_count < frame.detect_samples:\n",
    "            for car, box in frame.final_boxes.items():\n",
    "                cv2.rectangle(frame.image, box[0], box[1], (0, 0, 255), 4)   \n",
    "        else:\n",
    "            # Accumulate bounded boxes across several frames and then apply heat-map with threshold\n",
    "            frame.image, frame.final_boxes = make_labeled_bboxes(frame.image, frame.heat,\n",
    "                                                                 threshold=frame.detect_thresh, draw=True)\n",
    "            # Reset the store bboxes\n",
    "            frame.detect_samples = 10\n",
    "            frame.detect_count = 0\n",
    "            frame.heat = None\n",
    "            frame.find_cars = False       \n",
    "    else:\n",
    "        frame.track_count += 1\n",
    "        if frame.debug:\n",
    "            cv2.putText(frame.image, 'track count: {}'.format(str(frame.track_count)), (650, 175), font, 1, \n",
    "                        (200,255,155), 2, cv2.LINE_AA)\n",
    "            \n",
    "        adj_x = 15\n",
    "        adj_y = 5\n",
    "        for car, box in frame.final_boxes.items():\n",
    "            cv2.rectangle(frame.image, box[0], box[1], (0, 0, 255), 4)\n",
    "            ystart = np.clip(box[0][1] - adj_y, 0, frame.image.shape[0])\n",
    "            ystop = np.clip(box[1][1] + adj_y, 0, frame.image.shape[0])\n",
    "            xstart = np.clip(box[0][0] - adj_x, 0, frame.image.shape[1])\n",
    "            xstop = np.clip(box[1][0] + adj_x, 0, frame.image.shape[1])      \n",
    "            search_box += [((xstart, ystart), (xstop, ystop))]\n",
    "            single_car_boxes= []\n",
    "            scales = [0.8, 1.1, 1.6]       \n",
    "            for scale in scales:\n",
    "                _, bboxes = find_cars(image, img_format, xstart, xstop, ystart, ystop, scale, X_scaler,\n",
    "                                      svc, hog_dict, spatial_dict, hist_dict)\n",
    "                single_car_boxes += bboxes\n",
    "            \n",
    "            track_boxes += single_car_boxes\n",
    "            frame.car_boxes[car] += single_car_boxes\n",
    "            frame.heat = add_heat(frame.heat, single_car_boxes)\n",
    " \n",
    "        frame.heat_record.append(np.copy(frame.heat))\n",
    "        if frame.track_count >= frame.track_samples:\n",
    "            frame.track_count = 0\n",
    "            false_detections = [] # Detections that are false positives\n",
    "            low_conf_detections = [] # Detections that do not have a high enough overlap ratio with\n",
    "                                     # detection from the find phase\n",
    "            for car, boxes in frame.car_boxes.items():\n",
    "                heat = np.zeros_like(frame.image[:, :, 0])\n",
    "                heat = add_heat(heat, boxes)\n",
    "                test_car_boxes = make_labeled_bboxes(frame.image, heat, \n",
    "                                                     threshold=frame.track_thresh, draw=False)   \n",
    "                if test_car_boxes:\n",
    "                    org_box = frame.final_boxes[car]\n",
    "                    test_box = test_car_boxes[1]\n",
    "                    test_boxes += [test_box]\n",
    "                    # For there to be an intersection of boxes, min R edge > max L edge and \n",
    "                    # min bot edge > max top edge\n",
    "                    min_right_edge = min(test_box[1][0], org_box[1][0])\n",
    "                    max_left_edge = max(test_box[0][0], org_box[0][0])\n",
    "                    min_bottom_edge = min(test_box[1][1], org_box[1][1])\n",
    "                    max_top_edge = max(test_box[0][1], org_box[0][1])\n",
    "                    overlap = ((max_left_edge, max_top_edge), (min_right_edge, min_bottom_edge))\n",
    "                    overlap_area = (min_right_edge - max_left_edge) * (min_bottom_edge - max_top_edge)\n",
    "                    org_area = (org_box[1][0] - org_box[0][0]) * (org_box[1][1] - org_box[0][1])\n",
    "                    overlap_ratio = overlap_area / np.float(org_area)\n",
    "                    if ((min_right_edge > max_left_edge) & (min_bottom_edge > max_top_edge) & \n",
    "                        (overlap_ratio >= 0.7)):   \n",
    "                        frame.final_boxes[car] = test_box\n",
    "                    else:\n",
    "                        low_conf_detections += [car]                       \n",
    "                else:\n",
    "                    false_detections += [car]\n",
    "\n",
    "            frame.car_boxes = defaultdict(list)\n",
    "            if low_conf_detections:\n",
    "                frame.lost_count += 1\n",
    "                if frame.debug:\n",
    "                    text = 'Not enough overlap in cars: '\n",
    "                    for ii, car in enumerate(low_conf_detections):\n",
    "                        text += str(car)\n",
    "                        if ii < (len(low_conf_detections) - 1):\n",
    "                            text += ', '                  \n",
    "                    cv2.putText(frame.image, text, (650, 100), \n",
    "                                font, 1, (200,255,155), 2, cv2.LINE_AA)\n",
    "                    \n",
    "            if false_detections:\n",
    "                text = 'False positives in detections: '\n",
    "                for ii, car in enumerate(false_detections):\n",
    "                    frame.final_boxes.pop(car, None)\n",
    "                    if frame.debug:\n",
    "                        text += str(car)\n",
    "                        if ii < (len(false_detections) - 1):\n",
    "                            text += ', '   \n",
    "                cv2.putText(frame.image, text, (650, 125), font, 1, \n",
    "                            (200,255,155), 2, cv2.LINE_AA)\n",
    "                    \n",
    "#             if frame.lost_count >= frame.lost_thresh:\n",
    "#                 frame.find_cars = True\n",
    "#                 frame.heat = None\n",
    "#                 frame.detect_count = 0\n",
    "#                 frame.track_count = 0\n",
    "#                 frame.lost_count = 0\n",
    "#                 frame.detect_samples = 5\n",
    "#                 frame.detect_thresh = 6\n",
    "                            \n",
    "        if (frame.detect_count >= frame.detect_samples) | (frame.lost_count >= frame.lost_thresh):\n",
    "            frame.find_cars = True\n",
    "            frame.heat = None\n",
    "            frame.detect_count = 0\n",
    "            frame.track_count = 0\n",
    "            frame.lost_count = 0\n",
    "        \n",
    "    if frame.show_bboxes:\n",
    "        if frame.car_accum:\n",
    "            frame.show_bbox_details(thickness=[2, 4, 3, 2], \n",
    "                                    colors=[(100, 100, 0), (0, 0, 0), (0, 255, 255), (100, 0, 100)],\n",
    "                                    box_lists= [accum_detect_boxes, search_box, test_boxes, track_boxes])\n",
    "        else:\n",
    "            frame.show_bbox_details(thickness=[2, 4, 3, 2], \n",
    "                                    colors=[(100, 100, 0), (0, 0, 0), (0, 255, 255), (100, 0, 100)],\n",
    "                                    box_lists= [detect_boxes, search_box, test_boxes, track_boxes])\n",
    "    \n",
    "    frame.detect_count += 1\n",
    "    frame.frame_count += 1\n",
    "    \n",
    "    return frame.image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/1261 [00:00<?, ?it/s]\u001b[A\n",
      "  0%|          | 1/1261 [00:01<37:34,  1.79s/it]\u001b[A\n",
      "  0%|          | 2/1261 [00:03<37:01,  1.76s/it]\u001b[A\n",
      "  0%|          | 3/1261 [00:05<36:16,  1.73s/it]\u001b[A\n",
      "  0%|          | 4/1261 [00:06<35:39,  1.70s/it]\u001b[A\n",
      "  0%|          | 5/1261 [00:08<35:30,  1.70s/it]\u001b[A\n",
      "  0%|          | 6/1261 [00:10<35:06,  1.68s/it]\u001b[A\n",
      "  1%|          | 7/1261 [00:11<34:49,  1.67s/it]\u001b[A\n",
      "  1%|          | 8/1261 [00:13<34:38,  1.66s/it]\u001b[A\n",
      "  1%|          | 9/1261 [00:15<34:27,  1.65s/it]\u001b[A\n",
      "  1%|          | 10/1261 [00:16<34:20,  1.65s/it]\u001b[A\n",
      "  2%|▏         | 21/1261 [00:18<24:48,  1.20s/it]\u001b[A\n",
      "  2%|▏         | 22/1261 [00:20<27:35,  1.34s/it]\u001b[A\n",
      "  2%|▏         | 23/1261 [00:21<29:25,  1.43s/it]\u001b[A\n",
      "  2%|▏         | 24/1261 [00:23<30:41,  1.49s/it]\u001b[A\n",
      "  2%|▏         | 25/1261 [00:24<31:32,  1.53s/it]\u001b[A\n",
      "  2%|▏         | 26/1261 [00:26<32:11,  1.56s/it]\u001b[A\n",
      "  2%|▏         | 27/1261 [00:28<32:50,  1.60s/it]\u001b[A\n",
      "  2%|▏         | 28/1261 [00:29<33:15,  1.62s/it]\u001b[A\n",
      "  2%|▏         | 29/1261 [00:31<33:31,  1.63s/it]\u001b[A\n",
      "  2%|▏         | 30/1261 [00:33<33:43,  1.64s/it]\u001b[A\n",
      "  3%|▎         | 35/1261 [00:33<23:38,  1.16s/it]\u001b[A\n",
      "  3%|▎         | 41/1261 [00:35<18:11,  1.12it/s]\u001b[A\n",
      "  3%|▎         | 43/1261 [00:38<22:36,  1.11s/it]\u001b[A\n",
      "  3%|▎         | 44/1261 [00:39<25:45,  1.27s/it]\u001b[A\n",
      "  4%|▎         | 45/1261 [00:41<28:00,  1.38s/it]\u001b[A\n",
      "  4%|▎         | 46/1261 [00:43<29:31,  1.46s/it]\u001b[A\n",
      "  4%|▎         | 47/1261 [00:44<30:38,  1.51s/it]\u001b[A\n",
      "  4%|▍         | 48/1261 [00:46<31:19,  1.55s/it]\u001b[A\n",
      "  4%|▍         | 49/1261 [00:48<31:48,  1.57s/it]\u001b[A\n",
      "  4%|▍         | 50/1261 [00:49<32:09,  1.59s/it]\u001b[A\n",
      "  5%|▍         | 57/1261 [00:49<22:28,  1.12s/it]\u001b[A\n",
      "  5%|▍         | 61/1261 [00:51<18:16,  1.09it/s]\u001b[A\n",
      "  5%|▍         | 63/1261 [00:54<22:27,  1.12s/it]\u001b[A\n",
      "  5%|▌         | 65/1261 [00:58<25:21,  1.27s/it]\u001b[A\n",
      "  5%|▌         | 66/1261 [00:59<27:27,  1.38s/it]\u001b[A\n",
      "  5%|▌         | 67/1261 [01:01<29:00,  1.46s/it]\u001b[A\n",
      "  5%|▌         | 68/1261 [01:02<30:03,  1.51s/it]\u001b[A\n",
      "  5%|▌         | 69/1261 [01:04<30:42,  1.55s/it]\u001b[A\n",
      "  6%|▌         | 70/1261 [01:06<31:12,  1.57s/it]\u001b[A\n",
      "  6%|▌         | 78/1261 [01:06<21:47,  1.11s/it]\u001b[A\n",
      "  6%|▋         | 81/1261 [01:08<18:33,  1.06it/s]\u001b[A\n",
      "  7%|▋         | 83/1261 [01:11<22:28,  1.14s/it]\u001b[A\n",
      "  7%|▋         | 85/1261 [01:14<25:10,  1.28s/it]\u001b[A\n",
      "  7%|▋         | 86/1261 [01:16<27:33,  1.41s/it]\u001b[A\n",
      "  7%|▋         | 87/1261 [01:17<28:58,  1.48s/it]\u001b[A\n",
      "  7%|▋         | 88/1261 [01:19<29:48,  1.52s/it]\u001b[A\n",
      "  7%|▋         | 89/1261 [01:21<30:30,  1.56s/it]\u001b[A\n",
      "  7%|▋         | 90/1261 [01:22<30:57,  1.59s/it]\u001b[A\n",
      "  8%|▊         | 98/1261 [01:22<21:36,  1.11s/it]\u001b[A\n",
      "  8%|▊         | 101/1261 [01:24<18:24,  1.05it/s]\u001b[A\n",
      "  8%|▊         | 103/1261 [01:27<22:12,  1.15s/it]\u001b[A\n",
      "  8%|▊         | 105/1261 [01:31<24:50,  1.29s/it]\u001b[A\n",
      "  8%|▊         | 106/1261 [01:32<26:43,  1.39s/it]\u001b[A\n",
      "  8%|▊         | 107/1261 [01:34<28:05,  1.46s/it]\u001b[A\n",
      "  9%|▊         | 108/1261 [01:35<29:04,  1.51s/it]\u001b[A\n",
      "  9%|▊         | 109/1261 [01:37<29:43,  1.55s/it]\u001b[A\n",
      "  9%|▊         | 110/1261 [01:39<30:09,  1.57s/it]\u001b[A\n",
      "  9%|▉         | 118/1261 [01:39<21:03,  1.11s/it]\u001b[A\n",
      " 10%|▉         | 121/1261 [01:41<17:56,  1.06it/s]\u001b[A\n",
      " 10%|▉         | 123/1261 [01:44<21:42,  1.14s/it]\u001b[A\n",
      " 10%|▉         | 125/1261 [01:47<24:19,  1.29s/it]\u001b[A\n",
      " 10%|▉         | 126/1261 [01:49<26:14,  1.39s/it]\u001b[A\n",
      " 10%|█         | 127/1261 [01:50<27:35,  1.46s/it]\u001b[A\n",
      " 10%|█         | 128/1261 [01:52<28:29,  1.51s/it]\u001b[A\n",
      " 10%|█         | 129/1261 [01:53<29:08,  1.54s/it]\u001b[A\n",
      " 10%|█         | 130/1261 [01:55<29:42,  1.58s/it]\u001b[A\n",
      " 11%|█         | 133/1261 [01:55<20:58,  1.12s/it]\u001b[A\n",
      " 11%|█         | 141/1261 [01:57<15:48,  1.18it/s]\u001b[A\n",
      " 11%|█▏        | 142/1261 [01:59<20:08,  1.08s/it]\u001b[A\n",
      " 11%|█▏        | 143/1261 [02:00<23:12,  1.25s/it]\u001b[A\n",
      " 11%|█▏        | 144/1261 [02:02<25:19,  1.36s/it]\u001b[A\n",
      " 11%|█▏        | 145/1261 [02:04<26:46,  1.44s/it]\u001b[A\n",
      " 12%|█▏        | 146/1261 [02:05<27:52,  1.50s/it]\u001b[A\n",
      " 12%|█▏        | 147/1261 [02:07<28:31,  1.54s/it]\u001b[A\n",
      " 12%|█▏        | 148/1261 [02:08<29:00,  1.56s/it]\u001b[A\n",
      " 12%|█▏        | 149/1261 [02:10<29:19,  1.58s/it]\u001b[A\n",
      " 12%|█▏        | 150/1261 [02:12<29:33,  1.60s/it]\u001b[A\n",
      " 13%|█▎        | 158/1261 [02:12<20:36,  1.12s/it]\u001b[A\n",
      " 13%|█▎        | 161/1261 [02:13<17:29,  1.05it/s]\u001b[A\n",
      " 13%|█▎        | 163/1261 [02:17<21:03,  1.15s/it]\u001b[A\n",
      " 13%|█▎        | 165/1261 [02:20<23:37,  1.29s/it]\u001b[A\n",
      " 13%|█▎        | 166/1261 [02:22<25:24,  1.39s/it]\u001b[A\n",
      " 13%|█▎        | 167/1261 [02:23<26:41,  1.46s/it]\u001b[A\n",
      " 13%|█▎        | 168/1261 [02:25<27:33,  1.51s/it]\u001b[A\n",
      " 13%|█▎        | 169/1261 [02:26<28:12,  1.55s/it]\u001b[A\n",
      " 13%|█▎        | 170/1261 [02:28<28:41,  1.58s/it]\u001b[A\n",
      " 14%|█▎        | 171/1261 [02:28<20:40,  1.14s/it]\u001b[A\n",
      " 14%|█▎        | 172/1261 [02:28<15:04,  1.20it/s]\u001b[A\n",
      " 14%|█▎        | 173/1261 [02:28<11:11,  1.62it/s]\u001b[A\n",
      " 14%|█▍        | 174/1261 [02:29<08:25,  2.15it/s]\u001b[A\n",
      " 14%|█▍        | 175/1261 [02:29<06:28,  2.80it/s]\u001b[A\n",
      " 14%|█▍        | 176/1261 [02:29<05:10,  3.50it/s]\u001b[A\n",
      " 14%|█▍        | 177/1261 [02:29<04:12,  4.30it/s]\u001b[A\n",
      " 14%|█▍        | 178/1261 [02:29<03:31,  5.11it/s]\u001b[A\n",
      " 14%|█▍        | 179/1261 [02:29<03:06,  5.80it/s]\u001b[A\n",
      " 14%|█▍        | 180/1261 [02:31<11:05,  1.63it/s]\u001b[A\n",
      " 14%|█▍        | 181/1261 [02:32<16:32,  1.09it/s]\u001b[A\n",
      " 14%|█▍        | 182/1261 [02:34<20:19,  1.13s/it]\u001b[A\n",
      " 15%|█▍        | 183/1261 [02:36<23:00,  1.28s/it]\u001b[A\n",
      " 15%|█▍        | 184/1261 [02:37<24:48,  1.38s/it]\u001b[A\n",
      " 15%|█▍        | 185/1261 [02:39<26:05,  1.45s/it]\u001b[A\n",
      " 15%|█▍        | 186/1261 [02:41<26:58,  1.51s/it]\u001b[A\n",
      " 15%|█▍        | 187/1261 [02:42<27:35,  1.54s/it]\u001b[A\n",
      " 15%|█▍        | 188/1261 [02:44<28:01,  1.57s/it]\u001b[A\n",
      " 15%|█▍        | 189/1261 [02:45<28:19,  1.59s/it]\u001b[A\n",
      " 15%|█▌        | 191/1261 [02:46<20:17,  1.14s/it]\u001b[A\n",
      " 15%|█▌        | 192/1261 [02:46<14:45,  1.21it/s]\u001b[A\n",
      " 15%|█▌        | 194/1261 [02:46<10:49,  1.64it/s]\u001b[A\n",
      " 15%|█▌        | 195/1261 [02:46<08:08,  2.18it/s]\u001b[A\n",
      " 16%|█▌        | 196/1261 [02:46<06:15,  2.83it/s]\u001b[A\n",
      " 16%|█▌        | 197/1261 [02:46<04:57,  3.57it/s]\u001b[A\n",
      " 16%|█▌        | 198/1261 [02:46<04:04,  4.34it/s]\u001b[A\n",
      " 16%|█▌        | 199/1261 [02:46<03:25,  5.16it/s]\u001b[A\n",
      " 16%|█▌        | 200/1261 [02:48<11:03,  1.60it/s]\u001b[A\n",
      " 16%|█▌        | 201/1261 [02:50<16:23,  1.08it/s]\u001b[A\n",
      " 16%|█▌        | 202/1261 [02:51<20:16,  1.15s/it]\u001b[A\n",
      " 16%|█▌        | 203/1261 [02:53<23:07,  1.31s/it]\u001b[A\n",
      " 16%|█▌        | 204/1261 [02:55<24:47,  1.41s/it]\u001b[A\n",
      " 16%|█▋        | 205/1261 [02:56<25:54,  1.47s/it]\u001b[A\n",
      " 16%|█▋        | 206/1261 [02:58<26:40,  1.52s/it]\u001b[A\n",
      " 16%|█▋        | 207/1261 [03:00<27:13,  1.55s/it]\u001b[A\n",
      " 16%|█▋        | 208/1261 [03:01<27:39,  1.58s/it]\u001b[A\n",
      " 17%|█▋        | 209/1261 [03:03<27:58,  1.60s/it]\u001b[A\n",
      " 17%|█▋        | 210/1261 [03:03<20:09,  1.15s/it]\u001b[A\n",
      " 17%|█▋        | 211/1261 [03:03<14:41,  1.19it/s]\u001b[A\n",
      " 17%|█▋        | 212/1261 [03:03<10:55,  1.60it/s]\u001b[A\n",
      " 17%|█▋        | 213/1261 [03:03<08:14,  2.12it/s]\u001b[A\n",
      " 17%|█▋        | 214/1261 [03:03<06:21,  2.74it/s]\u001b[A\n",
      " 17%|█▋        | 215/1261 [03:04<05:06,  3.41it/s]\u001b[A\n",
      " 17%|█▋        | 216/1261 [03:04<04:09,  4.18it/s]\u001b[A\n",
      " 17%|█▋        | 217/1261 [03:04<03:31,  4.94it/s]\u001b[A\n",
      " 17%|█▋        | 218/1261 [03:04<03:06,  5.60it/s]\u001b[A\n",
      " 17%|█▋        | 219/1261 [03:04<02:46,  6.26it/s]\u001b[A\n",
      " 17%|█▋        | 220/1261 [03:06<10:32,  1.65it/s]\u001b[A\n",
      " 18%|█▊        | 221/1261 [03:07<15:51,  1.09it/s]\u001b[A\n",
      " 18%|█▊        | 222/1261 [03:09<19:38,  1.13s/it]\u001b[A\n",
      " 18%|█▊        | 223/1261 [03:11<22:12,  1.28s/it]\u001b[A\n",
      " 18%|█▊        | 224/1261 [03:12<24:01,  1.39s/it]\u001b[A\n",
      " 18%|█▊        | 225/1261 [03:14<25:13,  1.46s/it]\u001b[A\n",
      " 18%|█▊        | 226/1261 [03:16<26:24,  1.53s/it]\u001b[A\n",
      " 18%|█▊        | 227/1261 [03:17<27:01,  1.57s/it]\u001b[A\n",
      " 18%|█▊        | 228/1261 [03:19<27:19,  1.59s/it]\u001b[A\n",
      " 18%|█▊        | 229/1261 [03:20<27:33,  1.60s/it]\u001b[A\n",
      " 18%|█▊        | 230/1261 [03:21<20:25,  1.19s/it]\u001b[A\n",
      " 18%|█▊        | 231/1261 [03:21<15:25,  1.11it/s]\u001b[A\n",
      " 18%|█▊        | 232/1261 [03:21<11:58,  1.43it/s]\u001b[A\n",
      " 18%|█▊        | 233/1261 [03:21<09:28,  1.81it/s]\u001b[A\n",
      " 19%|█▊        | 234/1261 [03:22<07:43,  2.22it/s]\u001b[A\n",
      " 19%|█▊        | 235/1261 [03:22<06:31,  2.62it/s]\u001b[A\n",
      " 19%|█▊        | 236/1261 [03:22<05:40,  3.01it/s]\u001b[A\n",
      " 19%|█▉        | 237/1261 [03:22<05:03,  3.37it/s]\u001b[A\n",
      " 19%|█▉        | 238/1261 [03:22<04:41,  3.63it/s]\u001b[A\n",
      " 19%|█▉        | 239/1261 [03:23<04:21,  3.91it/s]\u001b[A\n",
      " 19%|█▉        | 240/1261 [03:24<11:24,  1.49it/s]\u001b[A\n",
      " 19%|█▉        | 241/1261 [03:26<16:16,  1.04it/s]\u001b[A\n",
      " 19%|█▉        | 242/1261 [03:28<19:38,  1.16s/it]\u001b[A\n",
      " 19%|█▉        | 243/1261 [03:29<22:03,  1.30s/it]\u001b[A\n",
      " 19%|█▉        | 244/1261 [03:31<23:41,  1.40s/it]\u001b[A\n",
      " 19%|█▉        | 245/1261 [03:32<24:49,  1.47s/it]\u001b[A\n",
      " 20%|█▉        | 246/1261 [03:34<25:35,  1.51s/it]\u001b[A\n",
      " 20%|█▉        | 247/1261 [03:36<26:09,  1.55s/it]\u001b[A\n",
      " 20%|█▉        | 248/1261 [03:37<26:33,  1.57s/it]\u001b[A\n",
      " 20%|█▉        | 249/1261 [03:39<26:53,  1.59s/it]\u001b[A\n",
      " 20%|█▉        | 250/1261 [03:39<19:52,  1.18s/it]\u001b[A\n",
      " 20%|█▉        | 251/1261 [03:39<14:58,  1.12it/s]\u001b[A\n",
      " 20%|█▉        | 252/1261 [03:40<11:38,  1.44it/s]\u001b[A\n",
      " 20%|██        | 253/1261 [03:40<09:09,  1.83it/s]\u001b[A\n",
      " 20%|██        | 254/1261 [03:40<07:25,  2.26it/s]\u001b[A\n",
      " 20%|██        | 255/1261 [03:40<06:17,  2.66it/s]\u001b[A\n",
      " 20%|██        | 256/1261 [03:40<05:28,  3.06it/s]\u001b[A\n",
      " 20%|██        | 257/1261 [03:41<04:53,  3.42it/s]\u001b[A\n",
      " 20%|██        | 258/1261 [03:41<04:34,  3.65it/s]\u001b[A\n",
      " 21%|██        | 259/1261 [03:41<04:18,  3.87it/s]\u001b[A\n",
      " 21%|██        | 260/1261 [03:43<11:17,  1.48it/s]\u001b[A\n",
      " 21%|██        | 261/1261 [03:44<16:02,  1.04it/s]\u001b[A\n",
      " 21%|██        | 262/1261 [03:46<19:19,  1.16s/it]\u001b[A\n",
      " 21%|██        | 263/1261 [03:48<21:37,  1.30s/it]\u001b[A\n",
      " 21%|██        | 264/1261 [03:49<23:14,  1.40s/it]\u001b[A\n",
      " 21%|██        | 265/1261 [03:51<24:22,  1.47s/it]\u001b[A\n",
      " 21%|██        | 266/1261 [03:53<25:08,  1.52s/it]\u001b[A\n",
      " 21%|██        | 267/1261 [03:54<25:40,  1.55s/it]\u001b[A\n",
      " 21%|██▏       | 268/1261 [03:56<26:03,  1.57s/it]\u001b[A\n",
      " 21%|██▏       | 269/1261 [03:57<26:21,  1.59s/it]\u001b[A\n",
      " 21%|██▏       | 270/1261 [03:58<19:52,  1.20s/it]\u001b[A\n",
      " 21%|██▏       | 271/1261 [03:58<15:18,  1.08it/s]\u001b[A"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "3",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-e6feeac9a0ef>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mclip1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVideoFileClip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"../\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mvideo_name\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"_video.mp4\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maudio\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mroad_clip\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclip1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfl_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_image\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmagic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'time road_clip.write_videofile(\"../\" + output_name + \"_output.mp4\", audio=False, verbose=0)'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/Beili/anaconda/envs/tensorflow/lib/python3.6/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mmagic\u001b[0;34m(self, arg_s)\u001b[0m\n\u001b[1;32m   2158\u001b[0m         \u001b[0mmagic_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmagic_arg_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marg_s\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartition\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2159\u001b[0m         \u001b[0mmagic_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmagic_name\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprefilter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mESC_MAGIC\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2160\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmagic_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2162\u001b[0m     \u001b[0;31m#-------------------------------------------------------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Beili/anaconda/envs/tensorflow/lib/python3.6/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_line_magic\u001b[0;34m(self, magic_name, line)\u001b[0m\n\u001b[1;32m   2079\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'local_ns'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf_locals\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2080\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2081\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2082\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2083\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<decorator-gen-60>\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n",
      "\u001b[0;32m/Users/Beili/anaconda/envs/tensorflow/lib/python3.6/site-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Beili/anaconda/envs/tensorflow/lib/python3.6/site-packages/IPython/core/magics/execution.py\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[1;32m   1179\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m'eval'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m             \u001b[0mst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1181\u001b[0;31m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_ns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1182\u001b[0m             \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1183\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<timed eval>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m<decorator-gen-173>\u001b[0m in \u001b[0;36mwrite_videofile\u001b[0;34m(self, filename, fps, codec, bitrate, audio, audio_fps, preset, audio_nbytes, audio_codec, audio_bitrate, audio_bufsize, temp_audiofile, rewrite_audio, remove_temp, write_logfile, verbose, threads, ffmpeg_params, progress_bar)\u001b[0m\n",
      "\u001b[0;32m/Users/Beili/anaconda/envs/tensorflow/lib/python3.6/site-packages/moviepy/decorators.py\u001b[0m in \u001b[0;36mrequires_duration\u001b[0;34m(f, clip, *a, **k)\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Attribute 'duration' not set\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclip\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<decorator-gen-172>\u001b[0m in \u001b[0;36mwrite_videofile\u001b[0;34m(self, filename, fps, codec, bitrate, audio, audio_fps, preset, audio_nbytes, audio_codec, audio_bitrate, audio_bufsize, temp_audiofile, rewrite_audio, remove_temp, write_logfile, verbose, threads, ffmpeg_params, progress_bar)\u001b[0m\n",
      "\u001b[0;32m/Users/Beili/anaconda/envs/tensorflow/lib/python3.6/site-packages/moviepy/decorators.py\u001b[0m in \u001b[0;36muse_clip_fps_by_default\u001b[0;34m(f, clip, *a, **k)\u001b[0m\n\u001b[1;32m    135\u001b[0m              for (k,v) in k.items()}\n\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclip\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mnew_a\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mnew_kw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<decorator-gen-171>\u001b[0m in \u001b[0;36mwrite_videofile\u001b[0;34m(self, filename, fps, codec, bitrate, audio, audio_fps, preset, audio_nbytes, audio_codec, audio_bitrate, audio_bufsize, temp_audiofile, rewrite_audio, remove_temp, write_logfile, verbose, threads, ffmpeg_params, progress_bar)\u001b[0m\n",
      "\u001b[0;32m/Users/Beili/anaconda/envs/tensorflow/lib/python3.6/site-packages/moviepy/decorators.py\u001b[0m in \u001b[0;36mconvert_masks_to_RGB\u001b[0;34m(f, clip, *a, **k)\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mclip\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mismask\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mclip\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclip\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_RGB\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclip\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m@\u001b[0m\u001b[0mdecorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecorator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Beili/anaconda/envs/tensorflow/lib/python3.6/site-packages/moviepy/video/VideoClip.py\u001b[0m in \u001b[0;36mwrite_videofile\u001b[0;34m(self, filename, fps, codec, bitrate, audio, audio_fps, preset, audio_nbytes, audio_codec, audio_bitrate, audio_bufsize, temp_audiofile, rewrite_audio, remove_temp, write_logfile, verbose, threads, ffmpeg_params, progress_bar)\u001b[0m\n\u001b[1;32m    347\u001b[0m                            \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthreads\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mthreads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m                            \u001b[0mffmpeg_params\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mffmpeg_params\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 349\u001b[0;31m                            progress_bar=progress_bar)\n\u001b[0m\u001b[1;32m    350\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mremove_temp\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mmake_audio\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Beili/anaconda/envs/tensorflow/lib/python3.6/site-packages/moviepy/video/io/ffmpeg_writer.py\u001b[0m in \u001b[0;36mffmpeg_write_video\u001b[0;34m(clip, filename, fps, codec, bitrate, preset, withmask, write_logfile, audiofile, verbose, threads, ffmpeg_params, progress_bar)\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m     for t,frame in clip.iter_frames(progress_bar=progress_bar, with_times=True,\n\u001b[0;32m--> 209\u001b[0;31m                                     fps=fps, dtype=\"uint8\"):\n\u001b[0m\u001b[1;32m    210\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mwithmask\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m             \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m255\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mclip\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Beili/anaconda/envs/tensorflow/lib/python3.6/site-packages/tqdm/_tqdm.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    831\u001b[0m \"\"\", fp_write=getattr(self.fp, 'write', sys.stderr.write))\n\u001b[1;32m    832\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 833\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    834\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    835\u001b[0m                 \u001b[0;31m# Update and print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Beili/anaconda/envs/tensorflow/lib/python3.6/site-packages/moviepy/Clip.py\u001b[0m in \u001b[0;36mgenerator\u001b[0;34m()\u001b[0m\n\u001b[1;32m    473\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mduration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mfps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 475\u001b[0;31m                 \u001b[0mframe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    476\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m                     \u001b[0mframe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mframe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<decorator-gen-136>\u001b[0m in \u001b[0;36mget_frame\u001b[0;34m(self, t)\u001b[0m\n",
      "\u001b[0;32m/Users/Beili/anaconda/envs/tensorflow/lib/python3.6/site-packages/moviepy/decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(f, *a, **kw)\u001b[0m\n\u001b[1;32m     87\u001b[0m         new_kw = {k: fun(v) if k in varnames else v\n\u001b[1;32m     88\u001b[0m                  for (k,v) in kw.items()}\n\u001b[0;32m---> 89\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnew_a\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mnew_kw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecorator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Beili/anaconda/envs/tensorflow/lib/python3.6/site-packages/moviepy/Clip.py\u001b[0m in \u001b[0;36mget_frame\u001b[0;34m(self, t)\u001b[0m\n\u001b[1;32m     93\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mframe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mapply_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeep_duration\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Beili/anaconda/envs/tensorflow/lib/python3.6/site-packages/moviepy/Clip.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m         \u001b[0;31m#mf = copy(self.make_frame)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m         \u001b[0mnewclip\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_make_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_frame\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mkeep_duration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Beili/anaconda/envs/tensorflow/lib/python3.6/site-packages/moviepy/video/VideoClip.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(gf, t)\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;31m`\u001b[0m\u001b[0mget_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mby\u001b[0m \u001b[0manother\u001b[0m \u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mimage_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m         \"\"\"\n\u001b[0;32m--> 533\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mgf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mimage_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mapply_to\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    534\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    535\u001b[0m     \u001b[0;31m# --------------------------------------------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-26-49de56a65d44>\u001b[0m in \u001b[0;36mprocess_image\u001b[0;34m(image)\u001b[0m\n\u001b[1;32m     93\u001b[0m                                                      threshold=frame.track_thresh, draw=False)   \n\u001b[1;32m     94\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtest_car_boxes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m                     \u001b[0morg_box\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mframe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinal_boxes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcar\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m                     \u001b[0mtest_box\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_car_boxes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m                     \u001b[0mtest_boxes\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtest_box\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 3"
     ]
    }
   ],
   "source": [
    "img_format = 'jpg'\n",
    "car_accum = False\n",
    "frame = Frame()\n",
    "frame.detect_samples = 10\n",
    "frame.track_samples = 3        \n",
    "frame.track_thresh = 3\n",
    "frame.show_bboxes = True\n",
    "frame.debug = True\n",
    "frame.car_accum = False\n",
    "if frame.car_accum:\n",
    "    frame.detect_thresh = 10\n",
    "else:\n",
    "    frame.detect_thresh = 9\n",
    "frame.single_car_boxes = []\n",
    "video_name = 'project'\n",
    "output_name = 'debug_project'\n",
    "clip1 = VideoFileClip(\"../\" + video_name + \"_video.mp4\", audio=False)\n",
    "road_clip = clip1.fl_image(process_image)\n",
    "%time road_clip.write_videofile(\"../\" + output_name + \"_output.mp4\", audio=False, verbose=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# print(len(frame.heat_record))\n",
    "fig, axes = plt.subplots(3, 3)\n",
    "# print(axes.ravel())\n",
    "dev = 2\n",
    "for i, ax in enumerate(axes.ravel()):\n",
    "    ax.imshow(frame.heat_record[i + dev], cmap='hot')\n",
    "    ax.set_title('Frame {}'.format(i + dev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1334e7e48>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fig = plt.figure()\n",
    "plt.imshow(frame.image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
