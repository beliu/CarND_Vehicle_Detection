{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Import the necessary libraries and packages\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n",
    "import glob\n",
    "import time\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from skimage.feature import hog\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import scipy.misc\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define a function to extract the target objects from the larger Udacity image and store them to a folder\n",
    "def extract_images(data, filepath):\n",
    "    for ii, img in enumerate(data):\n",
    "         # Read in each one by one\n",
    "        file = img[0]\n",
    "        image = mpimg.imread(file)\n",
    "        # Isolate the region in the image that contains the object\n",
    "        xmin, ymin, xmax, ymax = img[1], img[2], img[3], img[4]\n",
    "        image = image[ymin:ymax + 1, xmin:xmax + 1]\n",
    "        image = cv2.resize(image, (64, 64))\n",
    "        filler = '00000'\n",
    "        mpimg.imsave(filepath + 'image_' + filler[:len(filler) - len(str(ii))] + str(ii) + '.jpg', image)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convert_color(img, conv=None):\n",
    "    image = np.copy(img)\n",
    "    if conv != 'RGB':\n",
    "        transform = \"cv2.cvtColor(image, cv2.COLOR_\" + conv + \")\"\n",
    "        features = eval(transform)\n",
    "    else:\n",
    "        features = image\n",
    "    \n",
    "    return features\n",
    "    \n",
    "# Define a function to compute binned color features  \n",
    "def bin_spatial(img, size=(32, 32), channel='all'):\n",
    "\n",
    "    if channel == 'all':\n",
    "        channel = np.arange(3)\n",
    "    \n",
    "    features = []\n",
    "    for ch in channel:\n",
    "        color = cv2.resize(img[:, :, ch], size).ravel()\n",
    "        features.append(color)\n",
    "    spatial_features = np.concatenate(features)\n",
    "            \n",
    "    return spatial_features\n",
    "\n",
    "# Define a function to compute color histogram features  \n",
    "def color_hist(img, nbins=32, bins_range=(0, 256), channel='all'):\n",
    "    # Compute the histogram of the color channels separately\n",
    "    if channel == 'all':\n",
    "        channel = np.arange(3)\n",
    "        \n",
    "    features = []\n",
    "    hist_features = []\n",
    "    for ch in channel:\n",
    "        channel_hist = np.histogram(img[:, :, ch], bins=nbins, range=bins_range)\n",
    "        features.append(channel_hist[0])\n",
    "    hist_features = np.concatenate(features)\n",
    "        \n",
    "    # Return the individual histograms, bin_centers and feature vector\n",
    "    return hist_features\n",
    "\n",
    "# Define a function to return HOG features and visualization\n",
    "def get_hog_features(img, orient, pix_per_cell, cell_per_block, \n",
    "                        vis=False, feature_vec=True, trans_sqrt=True, block_norm='L1'):\n",
    "    # Call with two outputs if vis==True\n",
    "    if vis == True:\n",
    "        features, hog_image = hog(img, orientations=orient, pixels_per_cell=(pix_per_cell, pix_per_cell),\n",
    "                                  cells_per_block=(cell_per_block, cell_per_block), block_norm=block_norm,\n",
    "                                  transform_sqrt=trans_sqrt, visualise=vis, feature_vector=feature_vec)\n",
    "        return features, hog_image\n",
    "    # Otherwise call with one output\n",
    "    else:      \n",
    "        features = hog(img, orientations=orient, pixels_per_cell=(pix_per_cell, pix_per_cell),\n",
    "                       cells_per_block=(cell_per_block, cell_per_block), block_norm=block_norm,\n",
    "                       transform_sqrt=trans_sqrt, visualise=vis, feature_vector=feature_vec)\n",
    "        return features\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define a function to extract features from a list of images\n",
    "# Have this function call bin_spatial() and color_hist()\n",
    "def extract_features(data, img_format='png', spatial_dict=None, hist_dict=None, hog_dict=None):\n",
    "    \n",
    "    # Create a list to append feature vectors to\n",
    "    features = []\n",
    "    \n",
    "    # Iterate through the list of images\n",
    "    for file in data:\n",
    "        single_img_features = []\n",
    "        # Read in each one by one\n",
    "        image = mpimg.imread(file) \n",
    "        if ((img_format == 'jpg') | (file[-4:] == '.jpg')):\n",
    "            image = image.astype(np.float32) / 255\n",
    "        image = cv2.resize(image, (64, 64))\n",
    "        \n",
    "        if spatial_dict is not None:\n",
    "            cspace = spatial_dict['conv']\n",
    "            spatial_size = spatial_dict['size']\n",
    "            channels = spatial_dict['channels']\n",
    "            image_conv = convert_color(image, cspace)\n",
    "            spatial_features = bin_spatial(image_conv, size=(spatial_size, spatial_size), channel=channels)\n",
    "            single_img_features.append(spatial_features)\n",
    "            \n",
    "        if hist_dict is not None:\n",
    "            cspace = hist_dict['conv']\n",
    "            hist_bins = hist_dict['nbins']\n",
    "            bin_range = hist_dict['bin_range']\n",
    "            channels = hist_dict['channels']\n",
    "            image_conv = convert_color(image, cspace)\n",
    "            hist_features = color_hist(image_conv, nbins=hist_bins, bins_range=(0, 256), channel=channels)\n",
    "            single_img_features.append(hist_features)\n",
    "\n",
    "        if hog_dict is not None:\n",
    "            cspace = hog_dict['conv']\n",
    "            orient = hog_dict['orient']\n",
    "            pix_per_cell = hog_dict['pix_per_cell']\n",
    "            cell_per_block = hog_dict['cell_per_block']\n",
    "            channels = hog_dict['channels']       \n",
    "            trans_sqrt = hog_dict['trans_sqrt']\n",
    "            block_norm = hog_dict['block_norm']\n",
    "            image_conv = convert_color(image, cspace)  \n",
    "            hog_features = []\n",
    "            if channels == 'all':\n",
    "                channels = np.arange(3)\n",
    "            \n",
    "            for ch in channels:\n",
    "                hog_features.append(get_hog_features(image_conv[:,:,ch], \n",
    "                                    orient, pix_per_cell, cell_per_block, \n",
    "                                    vis=False, feature_vec=True, trans_sqrt=trans_sqrt, block_norm=block_norm))\n",
    "            hog_features = np.ravel(hog_features)                      \n",
    "            single_img_features.append(hog_features)\n",
    "            \n",
    "        features.append(np.concatenate(single_img_features))\n",
    "\n",
    "    # Return list of feature vectors\n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Save a dictionary into a pickle file\n",
    "def save_to_pickle(data, key_name, file_name):\n",
    "    if len(file_name) > 1:\n",
    "        for d, k, f in zip(data, key_name, file_name):\n",
    "            pickle_data = {k: d}\n",
    "            pickle.dump(pickle_data, open(f + '.p', \"wb\"))\n",
    "    else:\n",
    "        pickle_data = {}\n",
    "        for d, k in zip(data, key_name):\n",
    "            pickle_data[k] = d\n",
    "        print(pickle_data)\n",
    "        pickle.dump(pickle_data, open(file_name[0] + '.p', \"wb\"))\n",
    "    \n",
    "    return None\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_training_files(len_non_cars=None, GTI=False, KITTI=False, udacity=False):\n",
    "    \n",
    "    car_gti = []\n",
    "    kitti_cars = []\n",
    "    car_gti_far = []\n",
    "    car_gti_left = []\n",
    "    car_gti_midclose = []\n",
    "    car_gti_right = []\n",
    "    \n",
    "    if GTI:\n",
    "        # Collect the filenames for all car images in the dataset\n",
    "        car_gti_far = glob.glob('../data/vehicles/GTI_Far/*png')\n",
    "        car_gti_left = glob.glob('../data/vehicles/GTI_Left/*png')\n",
    "        car_gti_midclose = glob.glob('../data/vehicles/GTI_MiddleClose/*png')\n",
    "        car_gti_right = glob.glob('../data/vehicles/GTI_Right/*png')\n",
    "        car_gti = car_gti_far + car_gti_left + car_gti_midclose + car_gti_right\n",
    "    if KITTI:\n",
    "        kitti_cars = glob.glob('../data/vehicles/KITTI_extracted/*png')\n",
    "\n",
    "    # Collect the filenames for all non-car images in the dataset\n",
    "    non_car_gti = glob.glob('../data/non_vehicles/GTI/*png')\n",
    "    extras_non_cars = glob.glob('../data/non_vehicles/Extras/*png')\n",
    "    \n",
    "    car_files = np.array(car_gti + kitti_cars)\n",
    "    non_car_files = np.array(non_car_gti + extras_non_cars)\n",
    "    non_car_files = shuffle(non_car_files)\n",
    "    if len_non_cars: \n",
    "        n_sample = len_non_cars\n",
    "    else:\n",
    "        n_sample = len(non_car_files)\n",
    "    \n",
    "    udacity_car_files = np.array([])\n",
    "    if udacity:\n",
    "        udacity_df = pd.read_csv('../data/labels.csv')\n",
    "        udacity_cars_df = udacity_df.loc[(udacity_df.label == 'car') & (udacity_df.occluded == 0)]\n",
    "\n",
    "        udacity_car_idx = udacity_cars_df.index.tolist()\n",
    "        udacity_car_files = np.array(glob.glob('../data/udacity_cropped/*jpg'))\n",
    "        udacity_car_files = udacity_car_files[udacity_car_idx]\n",
    "\n",
    "    all_img_files = np.hstack((non_car_files[:n_sample], car_files, udacity_car_files))\n",
    "    img_type_labels = np.hstack((np.zeros(n_sample), np.ones(len(car_gti_far)), np.ones(len(car_gti_left)) * 2, \n",
    "                                     np.ones(len(car_gti_midclose)) * 3, np.ones(len(car_gti_right)) * 4, \n",
    "                                     np.ones(len(kitti_cars)) * 5, np.ones(len(udacity_car_files)) * 6))\n",
    "\n",
    "    print('No. of GTI car files: {}'.format(len(car_gti)))\n",
    "    print('No. of KITTI car files: {}'.format(len(kitti_cars)))\n",
    "    print('No. of Udacity car files: {}'.format(len(udacity_car_files)))\n",
    "    print('Total number of car files: {}'.format(len(car_files) + len(udacity_car_files)))\n",
    "    print('Tota number of non-car files: {}'.format(n_sample))\n",
    "    print('Total number of images: {}'.format(all_img_files.shape))\n",
    "    print('Total number of labels: {}'.format(img_type_labels.shape))\n",
    "\n",
    "    train_img_files = np.copy(all_img_files)\n",
    "    train_labels_copy = np.copy(img_type_labels)\n",
    "    \n",
    "    return train_img_files, train_labels_copy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_img_files, train_labels_copy = make_training_files(KITTI=True, len_non_cars=7000)\n",
    "\n",
    "img_format = 'png'\n",
    "## Here is a list of possible color spaces to use\n",
    "'''cv2.COLOR_RGB2HSV\n",
    "   cv2.COLOR_RGB2LUV\n",
    "   cv2.COLOR_RGB2HLS\n",
    "   cv2.COLOR_RGB2YUV\n",
    "   cv2.COLOR_RGB2YCrCb'''\n",
    "\n",
    "spatial_dict = {'conv': 'RGB2YCrCb', 'size': 32, 'channels': 'all'}\n",
    "hist_dict = {'conv': 'RGB2YCrCb', 'nbins': 32, 'bin_range': (0, 256), 'channels': 'all'}\n",
    "# spatial_dict = None\n",
    "# hist_dict = None\n",
    "hog_dict = {'conv': 'RGB2YCrCb', 'orient': 9, 'pix_per_cell': 8, 'cell_per_block': 2, 'channels': 'all',\n",
    "            'trans_sqrt': True, 'block_norm': 'L1'}\n",
    "\n",
    "X_train_features = extract_features(train_img_files, spatial_dict=spatial_dict, hist_dict=hist_dict, hog_dict=hog_dict)\n",
    "file_end = 'ycrcb_kitti_only'\n",
    "\n",
    "X_train_features = np.array(X_train_features).astype(np.float32)\n",
    "# Fit a per-column scaler\n",
    "X_scaler = StandardScaler().fit(X_train_features)\n",
    "# Apply the scaler to X\n",
    "scaled_X = X_scaler.transform(X_train_features)\n",
    "print('X train features shape is {}'.format(X_train_features.shape))\n",
    "print('Scaled X features is {}'.format(scaled_X.shape))\n",
    "\n",
    "save_to_pickle([scaled_X, train_labels_copy, X_scaler], ['features', 'labels', 'scaler'], \n",
    "               ['../pickle/scaled_X_' + file_end])\n",
    "save_to_pickle([spatial_dict, hist_dict, hog_dict], ['spatial', 'hist', 'hog'], \n",
    "               ['../pickle/dict_' + file_end])\n",
    "\n",
    "rand_state = np.random.randint(0, 100)\n",
    "# Shuffle and split the data into a training and test set\n",
    "X_train, X_validate, y_train, y_validate = train_test_split(scaled_X, train_labels_copy, test_size=0.2, \n",
    "                                                            stratify=train_labels_copy, random_state=rand_state)\n",
    "print('X_train shape is {}'.format(X_train.shape))\n",
    "print('y_train shape is {}'.format(y_train.shape))\n",
    "print('X_validate shape is {}'.format(X_validate.shape))\n",
    "print('y_validate shape is {}'.format(y_validate.shape))\n",
    "\n",
    "print(y_train[:20])\n",
    "y_train[y_train != 0] = 1.\n",
    "y_validate[y_validate != 0] = 1.\n",
    "print(y_train[:20])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Use a linear SVC \n",
    "svc = LinearSVC()\n",
    "# Check the training time for the SVC\n",
    "t=time.time()\n",
    "svc.fit(X_train, y_train)\n",
    "t2 = time.time()\n",
    "print(round(t2-t, 2), 'Seconds to train SVC...')\n",
    "print('Test Accuracy of SVC = ', round(svc.score(X_validate, y_validate), 4))\n",
    "save_to_pickle([svc, hog_dict['trans_sqrt'], hog_dict['block_norm']], ['svc', 'trans_sqrt', 'block_norm'], \n",
    "               ['../pickle/svc_pickle_' + file_end])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define a single function that can extract features using hog sub-sampling \n",
    "# and make predictions\n",
    "def find_cars(img, img_format, xstart, ystart, ystop, scale, X_scaler, svc, hog_dict, \n",
    "              spatial_dict=None, hist_dict=None):\n",
    "    \n",
    "    if spatial_dict is not None:\n",
    "        sp_cspace = spatial_dict['conv']\n",
    "        spatial_size = spatial_dict['size']\n",
    "        sp_ch = spatial_dict['channels']\n",
    "\n",
    "    if hist_dict is not None:\n",
    "        hs_cspace = hist_dict['conv']\n",
    "        hist_bins = hist_dict['nbins']\n",
    "        bin_range = hist_dict['bin_range']\n",
    "        hi_ch = hist_dict['channels']\n",
    "            \n",
    "    # Extract the Hog parameters\n",
    "    hg_cspace = hog_dict['conv']\n",
    "    orient = hog_dict['orient']\n",
    "    pix_per_cell = hog_dict['pix_per_cell']\n",
    "    cell_per_block = hog_dict['cell_per_block']\n",
    "    hg_ch = hog_dict['channels']       \n",
    "    trans_sqrt = hog_dict['trans_sqrt']\n",
    "    block_norm = hog_dict['block_norm']\n",
    "    \n",
    "    if hg_ch == 'all':\n",
    "        hg_ch = np.arange(3)\n",
    "    \n",
    "    draw_img = np.copy(img)\n",
    "    if img_format == 'jpg':\n",
    "        img = img.astype(np.float32) / 255\n",
    "     \n",
    "    # Compute individual channel HOG features for the entire image\n",
    "    img_tosearch = img[ystart:ystop,xstart:,:]\n",
    "    ctrans_tosearch = convert_color(img_tosearch, conv=hg_cspace)\n",
    "    bboxes = [] # Store the coordinates of the bounded boxes\n",
    "\n",
    "    if scale != 1:\n",
    "        imshape = ctrans_tosearch.shape\n",
    "        ctrans_tosearch = cv2.resize(ctrans_tosearch, (np.int(imshape[1] / scale), \n",
    "                                     np.int(imshape[0] / scale)))\n",
    "    chs = []\n",
    "    for ch in hg_ch:\n",
    "        chs.append(ctrans_tosearch[:,:,ch])\n",
    "\n",
    "    # Define blocks and steps\n",
    "    nxblocks = (chs[0].shape[1] // pix_per_cell) - cell_per_block + 1\n",
    "    nyblocks = (chs[0].shape[0] // pix_per_cell) - cell_per_block + 1 \n",
    "    nfeat_per_block = orient*cell_per_block**2\n",
    "\n",
    "    # 64 was the orginal sampling rate, with 8 cells and 8 pix per cell\n",
    "    window = 64\n",
    "    nblocks_per_window = (window // pix_per_cell) - cell_per_block + 1\n",
    "    cells_per_step = 2  # Instead of overlap, define how many cells to step\n",
    "    nxsteps = (nxblocks - nblocks_per_window) // cells_per_step + 1\n",
    "    nysteps = (nyblocks - nblocks_per_window) // cells_per_step + 1\n",
    "\n",
    "    hogs = []\n",
    "    for ch in chs:\n",
    "        hog_feature = get_hog_features(ch, orient, pix_per_cell, cell_per_block, \n",
    "                                feature_vec=False, trans_sqrt=trans_sqrt, block_norm=block_norm)\n",
    "        hogs.append(hog_feature)\n",
    "#     print(\"hog_feature shape: {}\".format(hogs[0].shape))\n",
    "    \n",
    "    for xb in range(nxsteps):\n",
    "        for yb in range(nysteps):\n",
    "            ypos = yb*cells_per_step\n",
    "            xpos = xb*cells_per_step\n",
    "\n",
    "            # Extract HOG for this patch\n",
    "            hog_list = []\n",
    "            for hg in hogs:    \n",
    "                hog_vect = hg[ypos:ypos+nblocks_per_window, \n",
    "                                 xpos:xpos+nblocks_per_window].ravel()\n",
    "                hog_list.append(hog_vect)   \n",
    "#             print(\"individual hog shape: {}\".format(hogs[0][ypos:ypos+nblocks_per_window, \n",
    "#                                  xpos:xpos+nblocks_per_window].shape))\n",
    "            hog_features = np.hstack(hog_list)\n",
    "        \n",
    "            xleft = xpos*pix_per_cell\n",
    "            ytop = ypos*pix_per_cell\n",
    "            \n",
    "#             print('original ctrans shape: {}'.format(imshape))\n",
    "#             print('resized ctrans shape: {}'.format(ctrans_tosearch.shape))\n",
    "#             print('xleft, xleft + window: {}, {}'.format(xleft, xleft + window))\n",
    "#             print('window ctrans shape: {}'.format(ctrans_tosearch[ytop:ytop+window, \n",
    "#                                                    xleft:xleft+window].shape))\n",
    "#             print('\\n')\n",
    "            \n",
    "           # Extract the image patch\n",
    "            subimg = cv2.resize(ctrans_tosearch[ytop:ytop+window, \n",
    "                                xleft:xleft+window], (64,64))\n",
    "\n",
    "            # Get color features\n",
    "            spatial_features = np.array([])\n",
    "            hist_features = np.array([])\n",
    "            if spatial_dict is not None:\n",
    "                spatial_features = bin_spatial(subimg, size=(spatial_size, spatial_size), channel=sp_ch)\n",
    "            if hist_dict is not None:\n",
    "                hist_features = color_hist(subimg, nbins=hist_bins, bins_range=(0, 256), channel=hi_ch)\n",
    "\n",
    "            # Scale features and make a prediction\n",
    "            features = np.hstack((spatial_features, hist_features, hog_features))\n",
    "            test_features = X_scaler.transform(features.reshape(1, -1))       \n",
    "            test_prediction = svc.predict(test_features)\n",
    "            \n",
    "            if test_prediction == 1:\n",
    "                xbox_left = np.int(xleft*scale)\n",
    "                ytop_draw = np.int(ytop*scale)\n",
    "                win_draw = np.int(window*scale)\n",
    "                cv2.rectangle(draw_img,(xbox_left+xstart, ytop_draw+ystart),\n",
    "                (xbox_left+xstart+win_draw,ytop_draw+win_draw+ystart),(0,0,255),6) \n",
    "                bboxes.append(((xbox_left + xstart, ytop_draw + ystart), \n",
    "                               (xbox_left + xstart + win_draw, ytop_draw + win_draw + ystart)))\n",
    "                \n",
    "    return draw_img, bboxes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_heat(heatmap, bbox_list):\n",
    "    # Iterate through list of bboxes\n",
    "    for box in bbox_list:\n",
    "        # Add += 1 for all pixels inside each bbox\n",
    "        # Assuming each \"box\" takes the form ((x1, y1), (x2, y2))\n",
    "        heatmap[box[0][1]:box[1][1], box[0][0]:box[1][0]] += 1\n",
    "\n",
    "    # Return updated heatmap\n",
    "    return heatmap\n",
    "\n",
    "def apply_threshold(heatmap, threshold):\n",
    "    # Zero out pixels below the threshold\n",
    "    heatmap[heatmap < threshold] = 0\n",
    "    # Return thresholded map\n",
    "    return heatmap\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.ndimage.measurements import label\n",
    "\n",
    "def draw_labeled_bboxes(img, labels):\n",
    "    out_bboxes = {} # A dict of all the bounded boxes for each car found in this mage\n",
    "    # Iterate through all detected cars\n",
    "    for car_number in range(1, labels[1]+1):\n",
    "        # Find pixels with each car_number label value\n",
    "        nonzero = (labels[0] == car_number).nonzero()\n",
    "        # Identify x and y values of those pixels\n",
    "        nonzeroy = np.array(nonzero[0])\n",
    "        nonzerox = np.array(nonzero[1])\n",
    "        # Define a bounding box based on min/max x and y\n",
    "        bbox = ((np.min(nonzerox), np.min(nonzeroy)), (np.max(nonzerox), np.max(nonzeroy)))\n",
    "        out_bboxes[car_number] = bbox\n",
    "        # Draw the box on the image\n",
    "        cv2.rectangle(img, bbox[0], bbox[1], (0,0,255), 3)\n",
    "        \n",
    "    # Return the image\n",
    "    return img, out_bboxes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from moviepy.editor import VideoFileClip\n",
    "from IPython.display import HTML\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define a class to keep track of data among frames\n",
    "class Frame():\n",
    "    def __init__(self):\n",
    "        self.count = 1 # Tracks the number of frames that have passed\n",
    "        self.bboxes = [] # Each entry in this list is the detection boxes for one frame; used for\n",
    "                         # accumulating detection boxes for heat map threshold\n",
    "        self.samples = 6 # How many frames should we collect boxes for\n",
    "        self.centroids = [] # A list of the centroids of each bounding box around a car\n",
    "        self.image = None\n",
    "        self.detect_boxes = None # An flattened version of self.bboxes\n",
    "        self.car_boxes = None # A list of the bounded box surrounding an entire car\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This is the function that processes each frame of the video to find cars and apply a box around them\n",
    "def process_image(image):\n",
    "    \n",
    "#     if frame.image is None:\n",
    "    frame.image = np.copy(image)\n",
    "    \n",
    "    far_bboxes = []\n",
    "    mid_bboxes = []\n",
    "    close_bboxes = []\n",
    "#     # Search for cars in the distance\n",
    "    xstart = 0\n",
    "#     ystart = 400\n",
    "#     ystop = 450\n",
    "#     scale = 0.5\n",
    "# #     print('far')\n",
    "#     _, far_bboxes = find_cars(frame.image, img_format, xstart, ystart, ystop, scale, X_scaler, \n",
    "#                               svc, hog_dict, spatial_dict, hist_dict)\n",
    "    # Search for cars in the middle\n",
    "    ystart = 400\n",
    "    ystop = 550\n",
    "    scale = 1\n",
    "#     print('mid')\n",
    "    _, mid_bboxes = find_cars(frame.image, img_format, xstart, ystart, ystop, scale, X_scaler, \n",
    "                              svc, hog_dict, spatial_dict, hist_dict)\n",
    "    # Search for close cars\n",
    "    ystart = 400\n",
    "    ystop = 650\n",
    "    scale = 1.5\n",
    "#     print('close')\n",
    "#     _, close_bboxes = find_cars(frame.image, img_format, xstart, ystart, ystop, scale, X_scaler, \n",
    "#                                 svc, hog_dict, spatial_dict, hist_dict)\n",
    "    total_bboxes = far_bboxes + mid_bboxes + close_bboxes\n",
    "    frame.bboxes.append(total_bboxes)\n",
    "           \n",
    "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "    cv2.putText(frame.image, str(len(frame.bboxes)), (650, 150), font, 1, (200,255,155), 2, cv2.LINE_AA)\n",
    "\n",
    "    if len(frame.bboxes) >= frame.samples:\n",
    "        frame.detect_boxes = [item for sublist in frame.bboxes for item in sublist]\n",
    "        \n",
    "        for box in frame.detect_boxes:\n",
    "            cv2.rectangle(frame.image, box[0], box[1], (255, 255, 0), 2)\n",
    "\n",
    "        # Accumulate bounded boxes across several frames and then apply heat-map with threshold\n",
    "        heat = np.zeros_like(image[:,:,0]).astype(np.float)\n",
    "        heat = add_heat(heat, frame.detect_boxes)\n",
    "        heat_map = apply_threshold(heat, 5)\n",
    "        labels = label(heat_map)\n",
    "        frame.image, frame.car_boxes = draw_labeled_bboxes(frame.image, labels)\n",
    "        # Reset the store bboxes\n",
    "        frame.bboxes = []\n",
    "    else:\n",
    "        if frame.detect_boxes is not None:\n",
    "            for box in frame.detect_boxes:\n",
    "                cv2.rectangle(image, box[0], box[1], (255, 255, 0), 2)\n",
    "        \n",
    "        if frame.car_boxes:\n",
    "            for car, box in frame.car_boxes.items():\n",
    "                cv2.rectangle(frame.image, box[0], box[1], (0, 0, 255), 2)\n",
    "                box_center = ((box[1][0] + box[0][0]) // 2, (box[1][1] + box[0][1]) // 2)\n",
    "                frame.centroids.append(box_center)\n",
    "        \n",
    "#     if frame.centroids:\n",
    "#         for center in frame.centroids:\n",
    "#             cv2.rectangle(frame.image, (center[0] - 2, center[1] - 2), (center[0] + 2, center[1] + 2), (255, 0, 0), 2)\n",
    "    \n",
    "    return frame.image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file_end = 'ycrcb'\n",
    "dict_pickle = pickle.load( open(\"../pickle/scaled_X_\" + file_end + \".p\", \"rb\") )\n",
    "X_scaler = dict_pickle['scaler']\n",
    "\n",
    "dict_pickle = pickle.load( open(\"../pickle/dict_\" + file_end + \".p\", \"rb\") )\n",
    "spatial_dict = dict_pickle['spatial']\n",
    "hist_dict = dict_pickle['hist']\n",
    "hog_dict = dict_pickle['hog']\n",
    "\n",
    "dict_pickle = pickle.load( open(\"../pickle/svc_pickle_\" + file_end + \".p\", \"rb\" ) )\n",
    "svc = dict_pickle['svc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 1260/1261 [12:00<00:00,  1.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 11min 44s, sys: 18.9 s, total: 12min 3s\n",
      "Wall time: 12min\n"
     ]
    }
   ],
   "source": [
    "img_format = 'jpg'\n",
    "frame = Frame()\n",
    "def count_frames(image):\n",
    "    result = np.copy(image)\n",
    "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "    cv2.putText(result, str(frame.count), (600, 150), font, 1, (200,255,155), 2, cv2.LINE_AA)\n",
    "    frame.count += 1\n",
    "    return result\n",
    "\n",
    "clip1 = VideoFileClip(\"../project_video.mp4\", audio=False)\n",
    "road_clip = clip1.fl_image(process_image)\n",
    "%time road_clip.write_videofile(\"../project_output.mp4\", audio=False, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
