{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "### Import the necessary libraries and packages\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n",
    "import glob\n",
    "import time\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.ndimage.measurements import label\n",
    "from skimage.feature import hog\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import scipy.misc\n",
    "from collections import defaultdict\n",
    "import PyQt5\n",
    "%matplotlib qt5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define a function to extract the target objects from the larger Udacity image and store them to a folder\n",
    "def extract_images(data, filepath):\n",
    "    for ii, img in enumerate(data):\n",
    "         # Read in each one by one\n",
    "        file = img[0]\n",
    "        image = mpimg.imread(file)\n",
    "        # Isolate the region in the image that contains the object\n",
    "        xmin, ymin, xmax, ymax = img[1], img[2], img[3], img[4]\n",
    "        image = image[ymin:ymax + 1, xmin:xmax + 1]\n",
    "        image = cv2.resize(image, (64, 64))\n",
    "        filler = '00000'\n",
    "        mpimg.imsave(filepath + 'image_' + filler[:len(filler) - len(str(ii))] + str(ii) + '.jpg', image)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convert_color(img, conv=None):\n",
    "    image = np.copy(img)\n",
    "    if conv != 'RGB':\n",
    "        transform = \"cv2.cvtColor(image, cv2.COLOR_\" + conv + \")\"\n",
    "        features = eval(transform)\n",
    "    else:\n",
    "        features = image\n",
    "    \n",
    "    return features\n",
    "    \n",
    "# Define a function to compute binned color features  \n",
    "def bin_spatial(img, size=(32, 32), channel='all'):\n",
    "    if channel == 'all':\n",
    "        channel = np.arange(3)\n",
    "    \n",
    "    features = []\n",
    "    for ch in channel:\n",
    "        color = cv2.resize(img[:, :, ch], size).ravel()\n",
    "        features.append(color)\n",
    "    spatial_features = np.concatenate(features)\n",
    "            \n",
    "    return spatial_features\n",
    "\n",
    "# Define a function to compute color histogram features  \n",
    "def color_hist(img, nbins=32, bins_range=(0, 256), channel='all'):   \n",
    "    # Compute the histogram of the color channels separately\n",
    "    if channel == 'all':\n",
    "        channel = np.arange(3)\n",
    "        \n",
    "    features = []\n",
    "    hist_features = []\n",
    "    for ch in channel:\n",
    "        channel_hist = np.histogram(img[:, :, ch], bins=nbins, range=bins_range)\n",
    "        features.append(channel_hist[0])\n",
    "    hist_features = np.concatenate(features)\n",
    "        \n",
    "    # Return the individual histograms, bin_centers and feature vector\n",
    "    return hist_features\n",
    "\n",
    "# Define a function to return HOG features and visualization\n",
    "def get_hog_features(img, orient, pix_per_cell, cell_per_block, \n",
    "                        vis=False, feature_vec=True, trans_sqrt=True, block_norm='L1'):\n",
    "    # Call with two outputs if vis==True\n",
    "    if vis == True:\n",
    "        features, hog_image = hog(img, orientations=orient, pixels_per_cell=(pix_per_cell, pix_per_cell),\n",
    "                                  cells_per_block=(cell_per_block, cell_per_block), block_norm=block_norm,\n",
    "                                  transform_sqrt=trans_sqrt, visualise=vis, feature_vector=feature_vec)\n",
    "        return features, hog_image\n",
    "    # Otherwise call with one output\n",
    "    else:      \n",
    "        features = hog(img, orientations=orient, pixels_per_cell=(pix_per_cell, pix_per_cell),\n",
    "                       cells_per_block=(cell_per_block, cell_per_block), block_norm=block_norm,\n",
    "                       transform_sqrt=trans_sqrt, visualise=vis, feature_vector=feature_vec)\n",
    "        return features\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define a function to extract features from a list of images\n",
    "# Have this function call bin_spatial() and color_hist()\n",
    "def extract_features(data, img_format='png', spatial_dict=None, hist_dict=None, hog_dict=None):\n",
    "    \n",
    "    # Create a list to append feature vectors to\n",
    "    features = []\n",
    "    \n",
    "    # Iterate through the list of images\n",
    "    for file in data:\n",
    "        single_img_features = []\n",
    "        # Read in each one by one\n",
    "        image = mpimg.imread(file) \n",
    "        if ((img_format == 'jpg') | (file[-4:] == '.jpg')):\n",
    "            image = image.astype(np.float32) / 255\n",
    "        image = cv2.resize(image, (64, 64))\n",
    "        \n",
    "        if spatial_dict is not None:\n",
    "            cspace = spatial_dict['conv']\n",
    "            spatial_size = spatial_dict['size']\n",
    "            channels = spatial_dict['channels']\n",
    "            image_conv = convert_color(image, cspace)\n",
    "            spatial_features = bin_spatial(image_conv, size=(spatial_size, spatial_size), channel=channels)\n",
    "            single_img_features.append(spatial_features)\n",
    "            \n",
    "        if hist_dict is not None:\n",
    "            cspace = hist_dict['conv']\n",
    "            hist_bins = hist_dict['nbins']\n",
    "            bin_range = hist_dict['bin_range']\n",
    "            channels = hist_dict['channels']\n",
    "            image_conv = convert_color(image, cspace)\n",
    "            hist_features = color_hist(image_conv, nbins=hist_bins, bins_range=(0, 256), channel=channels)\n",
    "            single_img_features.append(hist_features)\n",
    "\n",
    "        if hog_dict is not None:\n",
    "            cspace = hog_dict['conv']\n",
    "            orient = hog_dict['orient']\n",
    "            pix_per_cell = hog_dict['pix_per_cell']\n",
    "            cell_per_block = hog_dict['cell_per_block']\n",
    "            channels = hog_dict['channels']       \n",
    "            trans_sqrt = hog_dict['trans_sqrt']\n",
    "            block_norm = hog_dict['block_norm']\n",
    "            image_conv = convert_color(image, cspace)  \n",
    "            hog_features = []\n",
    "            if channels == 'all':\n",
    "                channels = np.arange(3)\n",
    "            \n",
    "            for ch in channels:\n",
    "                hog_features.append(get_hog_features(image_conv[:,:,ch], \n",
    "                                    orient, pix_per_cell, cell_per_block, \n",
    "                                    vis=False, feature_vec=True, trans_sqrt=trans_sqrt, block_norm=block_norm))\n",
    "            hog_features = np.ravel(hog_features)                      \n",
    "            single_img_features.append(hog_features)\n",
    "            \n",
    "        features.append(np.concatenate(single_img_features))\n",
    "\n",
    "    # Return list of feature vectors\n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Save a dictionary into a pickle file\n",
    "def save_to_pickle(data, key_name, file_name):\n",
    "    if len(file_name) > 1:\n",
    "        for d, k, f in zip(data, key_name, file_name):\n",
    "            pickle_data = {k: d}\n",
    "            pickle.dump(pickle_data, open(f + '.p', \"wb\"))\n",
    "    else:\n",
    "        pickle_data = {}\n",
    "        for d, k in zip(data, key_name):\n",
    "            pickle_data[k] = d\n",
    "        print(pickle_data)\n",
    "        pickle.dump(pickle_data, open(file_name[0] + '.p', \"wb\"))\n",
    "    \n",
    "    return None\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_training_files(GTI=False, KITTI=False, udacity=False, len_non_cars=None):\n",
    "    \n",
    "    car_gti = []\n",
    "    kitti_cars = []\n",
    "    car_gti_far = []\n",
    "    car_gti_left = []\n",
    "    car_gti_midclose = []\n",
    "    car_gti_right = []\n",
    "    \n",
    "    if GTI:\n",
    "        # Collect the filenames for all car images in the dataset\n",
    "        car_gti_far = glob.glob('../data/vehicles/GTI_Far/*png')\n",
    "        car_gti_left = glob.glob('../data/vehicles/GTI_Left/*png')\n",
    "        car_gti_midclose = glob.glob('../data/vehicles/GTI_MiddleClose/*png')\n",
    "        car_gti_right = glob.glob('../data/vehicles/GTI_Right/*png')\n",
    "        car_gti = car_gti_far + car_gti_left + car_gti_midclose + car_gti_right\n",
    "    if KITTI:\n",
    "        kitti_cars = glob.glob('../data/vehicles/KITTI_extracted/*png')\n",
    "\n",
    "    # Collect the filenames for all non-car images in the dataset\n",
    "    non_car_gti = glob.glob('../data/non_vehicles/GTI/*png')\n",
    "    extras_non_cars = glob.glob('../data/non_vehicles/Extras/*png')\n",
    "    \n",
    "    car_files = np.array(car_gti + kitti_cars)\n",
    "    non_car_files = np.array(non_car_gti + extras_non_cars)\n",
    "    non_car_files = shuffle(non_car_files)\n",
    "    if len_non_cars: \n",
    "        n_sample = len_non_cars\n",
    "    else:\n",
    "        n_sample = len(non_car_files)\n",
    "    \n",
    "    udacity_car_files = np.array([])\n",
    "    if udacity:\n",
    "        udacity_df = pd.read_csv('../data/labels.csv')\n",
    "        udacity_cars_df = udacity_df.loc[(udacity_df.label == 'car') & (udacity_df.occluded == 0)]\n",
    "\n",
    "        udacity_car_idx = udacity_cars_df.index.tolist()\n",
    "        udacity_car_files = np.array(glob.glob('../data/udacity_cropped/*jpg'))\n",
    "        udacity_car_files = udacity_car_files[udacity_car_idx]\n",
    "\n",
    "    all_img_files = np.hstack((non_car_files[:n_sample], car_files, udacity_car_files))\n",
    "    img_type_labels = np.hstack((np.zeros(n_sample), np.ones(len(car_gti_far)), np.ones(len(car_gti_left)) * 2, \n",
    "                                     np.ones(len(car_gti_midclose)) * 3, np.ones(len(car_gti_right)) * 4, \n",
    "                                     np.ones(len(kitti_cars)) * 5, np.ones(len(udacity_car_files)) * 6))\n",
    "\n",
    "    print('No. of GTI car files: {}'.format(len(car_gti)))\n",
    "    print('No. of KITTI car files: {}'.format(len(kitti_cars)))\n",
    "    print('No. of Udacity car files: {}'.format(len(udacity_car_files)))\n",
    "    print('Total number of car files: {}'.format(len(car_files) + len(udacity_car_files)))\n",
    "    print('Tota number of non-car files: {}'.format(n_sample))\n",
    "    print('Total number of images: {}'.format(all_img_files.shape))\n",
    "    print('Total number of labels: {}'.format(img_type_labels.shape))\n",
    "\n",
    "    train_img_files = np.copy(all_img_files)\n",
    "    train_labels_copy = np.copy(img_type_labels)\n",
    "    \n",
    "    return train_img_files, train_labels_copy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# train_img_files, train_labels_copy = make_training_files(GTI=True, KITTI=True, udacity=True)\n",
    "\n",
    "# img_format = 'png'\n",
    "# ## Here is a list of possible color spaces to use\n",
    "# '''cv2.COLOR_RGB2HSV\n",
    "#    cv2.COLOR_RGB2LUV\n",
    "#    cv2.COLOR_RGB2HLS\n",
    "#    cv2.COLOR_RGB2YUV\n",
    "#    cv2.COLOR_RGB2YCrCb'''\n",
    "\n",
    "# spatial_dict = {'conv': 'RGB2YCrCb', 'size': 32, 'channels': 'all'}\n",
    "# hist_dict = {'conv': 'RGB2YCrCb', 'nbins': 32, 'bin_range': (0, 256), 'channels': 'all'}\n",
    "# hog_dict = {'conv': 'RGB2YCrCb', 'orient': 9, 'pix_per_cell': 8, 'cell_per_block': 2, 'channels': 'all',\n",
    "#             'trans_sqrt': True, 'block_norm': 'L1'}\n",
    "\n",
    "# # X_train_features = extract_features(train_img_files, spatial_dict=spatial_dict, hist_dict=hist_dict, \n",
    "# #                                     hog_dict=hog_dict)\n",
    "# # file_end = 'ycrcb_massive_data'\n",
    "\n",
    "# # X_train_features = np.array(X_train_features).astype(np.float32)\n",
    "# # # Fit a per-column scaler\n",
    "# # X_scaler = StandardScaler().fit(X_train_features)\n",
    "# # # Apply the scaler to X\n",
    "# # scaled_X = X_scaler.transform(X_train_features)\n",
    "# print('X train features shape is {}'.format(X_train_features.shape))\n",
    "# print('Scaled X features is {}'.format(scaled_X.shape))\n",
    "\n",
    "# save_to_pickle([scaled_X, train_labels_copy, X_scaler], ['features', 'labels', 'scaler'], \n",
    "#                ['../pickle/scaled_X_' + file_end])\n",
    "# save_to_pickle([spatial_dict, hist_dict, hog_dict], ['spatial', 'hist', 'hog'], \n",
    "#                ['../pickle/dict_' + file_end])\n",
    "\n",
    "# rand_state = np.random.randint(0, 100)\n",
    "# # Shuffle and split the data into a training and test set\n",
    "# X_train, X_validate, y_train, y_validate = train_test_split(scaled_X, train_labels_copy, test_size=0.2, \n",
    "#                                                             stratify=train_labels_copy, random_state=rand_state)\n",
    "# print('X_train shape is {}'.format(X_train.shape))\n",
    "# print('y_train shape is {}'.format(y_train.shape))\n",
    "# print('X_validate shape is {}'.format(X_validate.shape))\n",
    "# print('y_validate shape is {}'.format(y_validate.shape))\n",
    "\n",
    "# print(y_train[:20])\n",
    "# y_train[y_train != 0] = 1.\n",
    "# y_validate[y_validate != 0] = 1.\n",
    "# print(y_train[:20])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Use a linear SVC \n",
    "# svc = LinearSVC()\n",
    "# # Check the training time for the SVC\n",
    "# t=time.time()\n",
    "# svc.fit(X_train, y_train)\n",
    "# t2 = time.time()\n",
    "# print(round(t2-t, 2), 'Seconds to train SVC...')\n",
    "# print('Test Accuracy of SVC = ', round(svc.score(X_validate, y_validate), 4))\n",
    "# save_to_pickle([svc, hog_dict['trans_sqrt'], hog_dict['block_norm']], ['svc', 'trans_sqrt', 'block_norm'], \n",
    "#                ['../pickle/svc_pickle_' + file_end])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file_end = 'ycrcb'\n",
    "dict_pickle = pickle.load( open(\"../pickle/scaled_X_\" + file_end + \".p\", \"rb\") )\n",
    "X_scaler = dict_pickle['scaler']\n",
    "\n",
    "dict_pickle = pickle.load( open(\"../pickle/dict_\" + file_end + \".p\", \"rb\") )\n",
    "spatial_dict = dict_pickle['spatial']\n",
    "hist_dict = dict_pickle['hist']\n",
    "hog_dict = dict_pickle['hog']\n",
    "\n",
    "dict_pickle = pickle.load( open(\"../pickle/svc_pickle_\" + file_end + \".p\", \"rb\" ) )\n",
    "svc = dict_pickle['svc']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define a single function that can extract features using hog sub-sampling \n",
    "# and make predictions\n",
    "def find_cars(img, img_format, xstart, xstop, ystart, ystop, scale, X_scaler, svc, hog_dict, \n",
    "              spatial_dict=None, hist_dict=None, print_pred=False):\n",
    "                \n",
    "    # Extract the Hog parameters\n",
    "    hg_cspace = hog_dict['conv']\n",
    "    orient = hog_dict['orient']\n",
    "    pix_per_cell = hog_dict['pix_per_cell']\n",
    "    cell_per_block = hog_dict['cell_per_block']\n",
    "    hg_ch = hog_dict['channels']       \n",
    "    trans_sqrt = hog_dict['trans_sqrt']\n",
    "    block_norm = hog_dict['block_norm']\n",
    "    \n",
    "    if hg_ch == 'all':\n",
    "        hg_ch = np.arange(3)\n",
    "    \n",
    "    draw_img = np.copy(img)\n",
    "    if img_format == 'jpg':\n",
    "        img = img.astype(np.float32) / 255\n",
    "     \n",
    "    # Compute individual channel HOG features for the entire image\n",
    "    img_tosearch = img[ystart:ystop, xstart:xstop, :]\n",
    "    ctrans_tosearch = convert_color(img_tosearch, conv=hg_cspace)\n",
    "    bboxes = [] # Store the coordinates of the bounded boxes\n",
    "\n",
    "    if scale != 1:\n",
    "        imshape = ctrans_tosearch.shape\n",
    "        ctrans_tosearch = cv2.resize(ctrans_tosearch, (np.int(imshape[1] / scale), \n",
    "                                     np.int(imshape[0] / scale)))\n",
    "    chs = []\n",
    "    for ch in hg_ch:\n",
    "        chs.append(ctrans_tosearch[:,:,ch])\n",
    "\n",
    "    # Define blocks and steps\n",
    "    nxblocks = (chs[0].shape[1] // pix_per_cell) - cell_per_block + 1\n",
    "    nyblocks = (chs[0].shape[0] // pix_per_cell) - cell_per_block + 1 \n",
    "    nfeat_per_block = orient*cell_per_block**2\n",
    "\n",
    "    # 64 was the orginal sampling rate, with 8 cells and 8 pix per cell\n",
    "    window = 64\n",
    "    nblocks_per_window = (window // pix_per_cell) - cell_per_block + 1\n",
    "    cells_per_step = 2  # Instead of overlap, define how many cells to step\n",
    "    nxsteps = (nxblocks - nblocks_per_window) // cells_per_step + 1\n",
    "    nysteps = (nyblocks - nblocks_per_window) // cells_per_step + 1\n",
    "\n",
    "    hogs = []\n",
    "    for ch in chs:\n",
    "        hog_feature = get_hog_features(ch, orient, pix_per_cell, cell_per_block, \n",
    "                                feature_vec=False, trans_sqrt=trans_sqrt, block_norm=block_norm)\n",
    "        hogs.append(hog_feature)\n",
    "    \n",
    "    for xb in range(nxsteps):\n",
    "        for yb in range(nysteps):\n",
    "            ypos = yb*cells_per_step\n",
    "            xpos = xb*cells_per_step\n",
    "\n",
    "            # Extract HOG for this patch\n",
    "            hog_list = []\n",
    "            for hg in hogs:    \n",
    "                hog_vect = hg[ypos:ypos+nblocks_per_window, \n",
    "                                 xpos:xpos+nblocks_per_window].ravel()\n",
    "                hog_list.append(hog_vect)   \n",
    "            hog_features = np.hstack(hog_list)\n",
    "        \n",
    "            xleft = xpos*pix_per_cell\n",
    "            ytop = ypos*pix_per_cell\n",
    "\n",
    "           # Extract the image patch\n",
    "            subimg = cv2.resize(ctrans_tosearch[ytop:ytop+window, \n",
    "                                xleft:xleft+window], (64,64))\n",
    "\n",
    "            # Get color features\n",
    "            spatial_features = np.array([])\n",
    "            hist_features = np.array([])\n",
    "            if spatial_dict is not None:\n",
    "                sp_cspace = spatial_dict['conv']\n",
    "                spatial_size = spatial_dict['size']\n",
    "                sp_ch = spatial_dict['channels']\n",
    "                spatial_features = bin_spatial(subimg, size=(spatial_size, spatial_size), channel=sp_ch)\n",
    "            if hist_dict is not None:\n",
    "                hs_cspace = hist_dict['conv']\n",
    "                hist_bins = hist_dict['nbins']\n",
    "                bin_range = hist_dict['bin_range']\n",
    "                hi_ch = hist_dict['channels']\n",
    "                hist_features = color_hist(subimg, nbins=hist_bins, bins_range=(0, 256), channel=hi_ch)\n",
    "\n",
    "            # Scale features and make a prediction\n",
    "            features = np.hstack((spatial_features, hist_features, hog_features))\n",
    "            test_features = X_scaler.transform(features.reshape(1, -1))       \n",
    "            test_prediction = svc.predict(test_features)\n",
    "            if print_pred:\n",
    "                print('prediction: {}'.format(test_prediction))\n",
    "            if test_prediction == 1:\n",
    "                xbox_left = np.int(xleft*scale)\n",
    "                ytop_draw = np.int(ytop*scale)\n",
    "                win_draw = np.int(window*scale)\n",
    "                cv2.rectangle(draw_img,(xbox_left+xstart, ytop_draw+ystart),\n",
    "                (xbox_left+xstart+win_draw,ytop_draw+win_draw+ystart),(0,0,255),6) \n",
    "                bboxes.append(((xbox_left + xstart, ytop_draw + ystart), \n",
    "                               (xbox_left + xstart + win_draw, ytop_draw + win_draw + ystart)))\n",
    "                \n",
    "    return draw_img, bboxes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_heat(heatmap, bbox_list):\n",
    "    # Iterate through list of bboxes\n",
    "    for box in bbox_list:\n",
    "        # Add += 1 for all pixels inside each bbox\n",
    "        # Assuming each \"box\" takes the form ((x1, y1), (x2, y2))\n",
    "        heatmap[box[0][1]:box[1][1], box[0][0]:box[1][0]] += 1\n",
    "\n",
    "    # Return updated heatmap\n",
    "    return heatmap\n",
    "\n",
    "def apply_threshold(heatmap, threshold):\n",
    "    # Zero out pixels below the threshold\n",
    "    output_heatmap = np.copy(heatmap)\n",
    "    output_heatmap[output_heatmap < threshold] = 0\n",
    "    \n",
    "    # Return thresholded map\n",
    "    return output_heatmap\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_labeled_bboxes(image, heat, threshold, draw, color=(0, 0, 255), thickness=3):\n",
    "    out_bboxes = {} # A dict of all the bounded boxes for each car found in this image\n",
    "    \n",
    "    # Accumulate bounded boxes across several frames and then apply heat-map with threshold\n",
    "    heat_map = apply_threshold(heat, threshold)\n",
    "    labels = label(heat_map)\n",
    "\n",
    "    # Iterate through all detected cars\n",
    "    for car_number in range(1, labels[1]+1):\n",
    "        # Find pixels with each car_number label value\n",
    "        nonzero = (labels[0] == car_number).nonzero()\n",
    "        # Identify x and y values of those pixels\n",
    "        nonzeroy = np.array(nonzero[0])\n",
    "        nonzerox = np.array(nonzero[1])\n",
    "        # Define a bounding box based on min/max x and y\n",
    "        bbox = ((np.min(nonzerox), np.min(nonzeroy)), (np.max(nonzerox), np.max(nonzeroy)))\n",
    "        out_bboxes[car_number] = bbox\n",
    "        if draw:\n",
    "            # Draw the box on the image\n",
    "            cv2.rectangle(image, bbox[0], bbox[1], color, thickness)\n",
    "    \n",
    "    if draw:\n",
    "        # Return the image and bboxes\n",
    "        return image, out_bboxes\n",
    "    else:\n",
    "        # Return the bboxes\n",
    "        return out_bboxes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from moviepy.editor import VideoFileClip\n",
    "from IPython.display import HTML\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define a class to keep track of data among frames\n",
    "class Frame():\n",
    "    def __init__(self):\n",
    "        self.image = None\n",
    "        self.frame_count = 0\n",
    "        self.car_accum = False\n",
    "        self.find_cars = True\n",
    "        self.show_bboxes = False\n",
    "        self.debug = False\n",
    "        self.detect_count = 0 # Tracks the number of frames that have passed\n",
    "        self.detect_samples = 10 # How many frames should we collect boxes for\n",
    "        self.detect_thresh = 9\n",
    "        self.track_count = 0\n",
    "        self.track_samples = 2        \n",
    "        self.track_thresh = 2\n",
    "        self.lost_count = 0\n",
    "        self.lost_thresh = 2\n",
    "        self.heat = None\n",
    "        self.heat_record = []\n",
    "        self.car_boxes = defaultdict(list)\n",
    "        self.final_boxes = defaultdict(list)\n",
    "\n",
    "    def show_bbox_details(self, thickness, colors, box_lists):\n",
    "        for t, color, box_list in zip(thickness, colors, box_lists):\n",
    "            for box in box_list:\n",
    "                cv2.rectangle(self.image, box[0], box[1], color, t)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 not in B\n"
     ]
    }
   ],
   "source": [
    "A = {1: 1, 2: 2}\n",
    "B = {1: 1}\n",
    "for car in A.keys():\n",
    "    if car not in B:\n",
    "        print('{} not in B'.format(car))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This is the function that processes each frame of the video to find cars and apply a box around them\n",
    "def process_image(image):\n",
    "    \n",
    "    frame.image = np.copy(image)\n",
    "    if frame.heat is None:\n",
    "        frame.heat = np.zeros_like(frame.image[:, :, 0])\n",
    "    if frame.debug:\n",
    "        font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "        cv2.putText(frame.image, 'detect count: {}'.format(str(frame.detect_count)), (650, 150), font, 1, \n",
    "                    (200,255,155), 2, cv2.LINE_AA)\n",
    "        cv2.putText(frame.image, 'frame count: {}'.format(frame.frame_count), (650, 200), font, 1, \n",
    "                    (200,255,155), 2, cv2.LINE_AA)\n",
    "    \n",
    "    track_boxes = []\n",
    "    test_boxes = []\n",
    "    detect_boxes = []\n",
    "    accum_detect_boxes = []\n",
    "    search_box = []\n",
    "    car_boxes = []\n",
    "    \n",
    "    if frame.find_cars:\n",
    "        # Search for cars in the distance\n",
    "        xstart = 0\n",
    "        xstop = image.shape[1]\n",
    "        ycoords = [(400, 450), (400, 550), (400, 650)]\n",
    "        scales = [0.5, 1, 1.5]\n",
    "        for ycoord, scale in zip(ycoords, scales):\n",
    "            ystart, ystop = ycoord[0], ycoord[1]\n",
    "            _, bboxes = find_cars(image, img_format, xstart, xstop, ystart, ystop, scale, X_scaler,\n",
    "                                  svc, hog_dict, spatial_dict, hist_dict)\n",
    "            detect_boxes += bboxes\n",
    "        \n",
    "        if frame.car_accum:\n",
    "            # Accumulate bounded boxes across several frames and then apply heat-map with threshold\n",
    "            heat = np.zeros_like(frame.image[:, :, 0])\n",
    "            heat = add_heat(heat, detect_boxes)\n",
    "            car_boxes = make_labeled_bboxes(frame.image, heat, threshold=1, draw=False)\n",
    "            for car, box in car_boxes.items():\n",
    "                accum_detect_boxes += [box]\n",
    "#                 frame.car_boxes[car] += list([box])\n",
    "            frame.heat = add_heat(frame.heat, accum_detect_boxes)\n",
    "        else:\n",
    "            frame.heat = add_heat(frame.heat, detect_boxes)\n",
    "        \n",
    "        frame.heat_record.append(np.copy(frame.heat))\n",
    "        if frame.detect_count < frame.detect_samples:\n",
    "            for car, box in frame.final_boxes.items():\n",
    "                cv2.rectangle(frame.image, box[0], box[1], (0, 0, 255), 4)   \n",
    "        else:\n",
    "            # Accumulate bounded boxes across several frames and then apply heat-map with threshold\n",
    "            frame.image, frame.final_boxes = make_labeled_bboxes(frame.image, frame.heat,\n",
    "                                                                 threshold=frame.detect_thresh, draw=True)\n",
    "            # Reset the store bboxes\n",
    "            frame.detect_samples = 10\n",
    "            frame.detect_count = 0\n",
    "            frame.heat = None\n",
    "            frame.find_cars = False\n",
    "        \n",
    "    else:\n",
    "        frame.track_count += 1\n",
    "        if frame.debug:\n",
    "            cv2.putText(frame.image, 'track count: {}'.format(str(frame.track_count)), (650, 175), font, 1, \n",
    "                        (200,255,155), 2, cv2.LINE_AA)\n",
    "        adj_x = 15\n",
    "        adj_y = 5\n",
    "        for car, box in frame.final_boxes.items():\n",
    "            cv2.rectangle(frame.image, box[0], box[1], (0, 0, 255), 4)\n",
    "            ystart = np.clip(box[0][1] - adj_y, 0, frame.image.shape[0])\n",
    "            ystop = np.clip(box[1][1] + adj_y, 0, frame.image.shape[0])\n",
    "            xstart = np.clip(box[0][0] - adj_x, 0, frame.image.shape[1])\n",
    "            xstop = np.clip(box[1][0] + adj_x, 0, frame.image.shape[1])      \n",
    "            search_box += [((xstart, ystart), (xstop, ystop))]\n",
    "            single_car_boxes= []\n",
    "            scales = [0.8, 1.1, 1.6]\n",
    "            \n",
    "            for scale in scales:\n",
    "                _, bboxes = find_cars(image, img_format, xstart, xstop, ystart, ystop, scale, X_scaler,\n",
    "                                      svc, hog_dict, spatial_dict, hist_dict)\n",
    "                single_car_boxes += bboxes\n",
    "#                 frame.single_car_boxes.append(bboxes)\n",
    "            \n",
    "#             colors = [(255, 0, 0), (0, 255, 0), (0, 0, 255)]\n",
    "#             for color, bboxes in zip(colors, frame.single_car_boxes):\n",
    "#                 for box in bboxes:\n",
    "#                     cv2.rectangle(frame.image, box[0], box[1], color, 2)\n",
    "#             frame.single_car_boxes = []\n",
    "            \n",
    "            track_boxes += single_car_boxes\n",
    "            frame.heat = add_heat(frame.heat, single_car_boxes)\n",
    " \n",
    "        frame.heat_record.append(np.copy(frame.heat))\n",
    "        if frame.track_count >= frame.track_samples:\n",
    "            frame.track_count = 0\n",
    "            test_car_boxes = make_labeled_bboxes(frame.image, frame.heat, threshold=frame.track_thresh, draw=False)\n",
    "            \n",
    "            if len(test_car_boxes) == len(frame.final_boxes):\n",
    "                for car, org_box in frame.final_boxes.items():\n",
    "                    \n",
    "                    if car in test_car_boxes:\n",
    "                        test_box = test_car_boxes[car]\n",
    "                        test_boxes += [test_box]\n",
    "                        # For there to be an intersection of boxes, min R edge > max L edge and \n",
    "                        # min bot edge > max top edge\n",
    "                        min_right_edge = min(test_box[1][0], org_box[1][0])\n",
    "                        max_left_edge = max(test_box[0][0], org_box[0][0])\n",
    "                        min_bottom_edge = min(test_box[1][1], org_box[1][1])\n",
    "                        max_top_edge = max(test_box[0][1], org_box[0][1])\n",
    "                        overlap = ((max_left_edge, max_top_edge), (min_right_edge, min_bottom_edge))\n",
    "                        overlap_area = (min_right_edge - max_left_edge) * (min_bottom_edge - max_top_edge)\n",
    "                        org_area = (org_box[1][0] - org_box[0][0]) * (org_box[1][1] - org_box[0][1])\n",
    "                        overlap_ratio = overlap_area / np.float(org_area)\n",
    "                        if ((min_right_edge > max_left_edge) & (min_bottom_edge > max_top_edge) & \n",
    "                            (overlap_ratio >= 0.7)):   \n",
    "                            frame.final_boxes[car] = test_box\n",
    "                        else:\n",
    "                            frame.lost_count += 1\n",
    "                            if frame.debug:\n",
    "                                cv2.putText(frame.image, 'Hi Conf Box Not Found for Car {}'.format(car), (650, 100), \n",
    "                                            font, 1, (200,255,155), 2, cv2.LINE_AA)\n",
    "                                cv2.putText(frame.image, 'ratio: {}'.format(str(overlap_ratio)), (650, 125), font, 1, \n",
    "                                            (200,255,155), 2, cv2.LINE_AA)\n",
    "            else:\n",
    "                frame.lost_count += 1\n",
    "                problem_detections = []\n",
    "                for car in frame.final_boxes.keys():\n",
    "                    if car not in test_car_boxes:\n",
    "                        problem_detections += [car]\n",
    "                for car in problem_detections:\n",
    "                    frame.final_boxes.pop(car, None)\n",
    "                    if frame.debug:\n",
    "                        cv2.putText(frame.image, 'Glitch Detection on {}'.format(car), (650, 125), font, 1, \n",
    "                                    (200,255,155), 2, cv2.LINE_AA)\n",
    "                    \n",
    "#             if frame.lost_count >= frame.lost_thresh:\n",
    "#                 frame.find_cars = True\n",
    "#                 frame.heat = None\n",
    "#                 frame.detect_count = 0\n",
    "#                 frame.track_count = 0\n",
    "#                 frame.lost_count = 0\n",
    "#                 frame.detect_samples = 5\n",
    "#                 frame.detect_thresh = 6\n",
    "                            \n",
    "        if (frame.detect_count >= frame.detect_samples) | (frame.lost_count >= frame.lost_thresh):\n",
    "            frame.find_cars = True\n",
    "            frame.heat = None\n",
    "            frame.detect_count = 0\n",
    "            frame.track_count = 0\n",
    "            frame.lost_count = 0\n",
    "        \n",
    "    if frame.show_bboxes:\n",
    "        if frame.car_accum:\n",
    "            frame.show_bbox_details(thickness=[2, 4, 3, 2], \n",
    "                                    colors=[(100, 100, 0), (0, 0, 0), (0, 255, 255), (100, 0, 100)],\n",
    "                                    box_lists= [accum_detect_boxes, search_box, test_boxes, track_boxes])\n",
    "        else:\n",
    "            frame.show_bbox_details(thickness=[2, 4, 3, 2], \n",
    "                                    colors=[(100, 100, 0), (0, 0, 0), (0, 255, 255), (100, 0, 100)],\n",
    "                                    box_lists= [detect_boxes, search_box, test_boxes, track_boxes])\n",
    "    \n",
    "    frame.detect_count += 1\n",
    "    frame.frame_count += 1\n",
    "    \n",
    "    return frame.image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 1260/1261 [20:19<00:01,  1.51s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 19min 57s, sys: 27 s, total: 20min 24s\n",
      "Wall time: 20min 19s\n"
     ]
    }
   ],
   "source": [
    "img_format = 'jpg'\n",
    "car_accum = False\n",
    "frame = Frame()\n",
    "frame.detect_samples = 10\n",
    "frame.track_samples = 3        \n",
    "frame.track_thresh = 3\n",
    "frame.show_bboxes = True\n",
    "frame.debug = True\n",
    "frame.car_accum = True\n",
    "if frame.car_accum:\n",
    "    frame.detect_thresh = 10\n",
    "else:\n",
    "    frame.detect_thresh = 10\n",
    "frame.single_car_boxes = []\n",
    "video_name = 'project'\n",
    "output_name = 'debug_project'\n",
    "clip1 = VideoFileClip(\"../\" + video_name + \"_video.mp4\", audio=False)\n",
    "road_clip = clip1.fl_image(process_image)\n",
    "%time road_clip.write_videofile(\"../\" + output_name + \"_output.mp4\", audio=False, verbose=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# print(len(frame.heat_record))\n",
    "fig, axes = plt.subplots(4, 3)\n",
    "# print(axes.ravel())\n",
    "dev = 1006\n",
    "for i, ax in enumerate(axes.ravel()):\n",
    "    ax.imshow(frame.heat_record[i + dev], cmap='hot')\n",
    "    ax.set_title('Frame {}'.format(i + dev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
