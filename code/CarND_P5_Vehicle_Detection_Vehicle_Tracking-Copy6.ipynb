{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "### Import the necessary libraries and packages\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n",
    "import glob\n",
    "import time\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.ndimage.measurements import label\n",
    "from skimage.feature import hog\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "from scipy import ndimage\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import scipy.misc\n",
    "import random\n",
    "from collections import defaultdict\n",
    "from collections import Counter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# udacity_df = pd.read_csv('../data/labels.csv')\n",
    "# udacity_cars_df = udacity_df.loc[(udacity_df.label == 'car') & (udacity_df.occluded == 0)]\n",
    "# udacity_cars = udacity_cars_df.as_matrix()\n",
    "# extract_images(udacity_cars, '../data/Udacity_dataset/', '../data/udacity_cropped/')\n",
    "\n",
    "# udacity_car_files = glob.glob('../data/udacity_cropped/*jpg')\n",
    "# plot_images(udacity_car_files[-200:-190], 5, 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_images(files, rows, cols):\n",
    "    fig, axes = plt.subplots(rows, cols)\n",
    "    plt.tight_layout()\n",
    "    # plt.suptitle(title, fontsize=10)\n",
    "    for i, ax in enumerate(axes.ravel()):\n",
    "        image = mpimg.imread(files[i])\n",
    "        ax.imshow(image)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define a function to extract the target objects from the larger Udacity image and store them to a folder\n",
    "def extract_images(data, data_path, save_path):\n",
    "    for ii, img in enumerate(data):\n",
    "         # Read in each one by one\n",
    "        file = data_path + img[0]\n",
    "        image = mpimg.imread(file)\n",
    "        # Isolate the region in the image that contains the object\n",
    "        xmin, ymin, xmax, ymax = img[1], img[2], img[3], img[4]\n",
    "        image = image[ymin:ymax + 1, xmin:xmax + 1]\n",
    "        image = cv2.resize(image, (64, 64))\n",
    "        filler = '00000'\n",
    "        mpimg.imsave(save_path + 'image_' + filler[:len(filler) - len(str(ii + 1))] + str(ii + 1) + '.png', image)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convert_color(img, conv=None):\n",
    "    image = np.copy(img)\n",
    "    if conv != 'RGB':\n",
    "        transform = \"cv2.cvtColor(image, cv2.COLOR_\" + conv + \")\"\n",
    "        features = eval(transform)\n",
    "    else:\n",
    "        features = image\n",
    "    \n",
    "    return features\n",
    "    \n",
    "# Define a function to compute binned color features  \n",
    "def bin_spatial(img, size=(32, 32), channel='all'):\n",
    "    if channel == 'all':\n",
    "        channel = np.arange(3)\n",
    "    \n",
    "    features = []\n",
    "    for ch in channel:\n",
    "        color = cv2.resize(img[:, :, ch], size).ravel()\n",
    "        features.append(color)\n",
    "    spatial_features = np.concatenate(features)\n",
    "            \n",
    "    return spatial_features\n",
    "\n",
    "# Define a function to compute color histogram features  \n",
    "def color_hist(img, nbins=32, bins_range=(0, 256), channel='all'):   \n",
    "    # Compute the histogram of the color channels separately\n",
    "    if channel == 'all':\n",
    "        channel = np.arange(3)\n",
    "        \n",
    "    features = []\n",
    "    hist_features = []\n",
    "    for ch in channel:\n",
    "        channel_hist = np.histogram(img[:, :, ch], bins=nbins, range=bins_range)\n",
    "        features.append(channel_hist[0])\n",
    "    hist_features = np.concatenate(features)\n",
    "        \n",
    "    # Return the individual histograms, bin_centers and feature vector\n",
    "    return hist_features\n",
    "\n",
    "# Define a function to return HOG features and visualization\n",
    "def get_hog_features(img, orient, pix_per_cell, cell_per_block, \n",
    "                        vis=False, feature_vec=True, trans_sqrt=True, block_norm='L1'):\n",
    "    # Call with two outputs if vis==True\n",
    "    if vis == True:\n",
    "        features, hog_image = hog(img, orientations=orient, pixels_per_cell=(pix_per_cell, pix_per_cell),\n",
    "                                  cells_per_block=(cell_per_block, cell_per_block), block_norm=block_norm,\n",
    "                                  transform_sqrt=trans_sqrt, visualise=vis, feature_vector=feature_vec)\n",
    "        return features, hog_image\n",
    "    # Otherwise call with one output\n",
    "    else:      \n",
    "        features = hog(img, orientations=orient, pixels_per_cell=(pix_per_cell, pix_per_cell),\n",
    "                       cells_per_block=(cell_per_block, cell_per_block), block_norm=block_norm,\n",
    "                       transform_sqrt=trans_sqrt, visualise=vis, feature_vector=feature_vec)\n",
    "        return features\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define a function to extract features from a list of images\n",
    "# Have this function call bin_spatial() and color_hist()\n",
    "def extract_features(data, img_format='png', spatial_dict=None, hist_dict=None, hog_dict=None):\n",
    "    \n",
    "    # Create a list to append feature vectors to\n",
    "    features = []\n",
    "    \n",
    "    # Iterate through the list of images\n",
    "    for file in data:\n",
    "        single_img_features = []\n",
    "        # Read in each one by one\n",
    "        image = mpimg.imread(file) \n",
    "        if ((img_format == 'jpg') | (file[-4:] == '.jpg')):\n",
    "            image = image.astype(np.float32) / 255\n",
    "        image = cv2.resize(image, (64, 64))\n",
    "        \n",
    "        if spatial_dict is not None:\n",
    "            cspace = spatial_dict['conv']\n",
    "            spatial_size = spatial_dict['size']\n",
    "            channels = spatial_dict['channels']\n",
    "            image_conv = convert_color(image, cspace)\n",
    "            spatial_features = bin_spatial(image_conv, size=(spatial_size, spatial_size), channel=channels)\n",
    "            single_img_features.append(spatial_features)\n",
    "            \n",
    "        if hist_dict is not None:\n",
    "            cspace = hist_dict['conv']\n",
    "            hist_bins = hist_dict['nbins']\n",
    "            bin_range = hist_dict['bin_range']\n",
    "            channels = hist_dict['channels']\n",
    "            image_conv = convert_color(image, cspace)\n",
    "            hist_features = color_hist(image_conv, nbins=hist_bins, bins_range=(0, 256), channel=channels)\n",
    "            single_img_features.append(hist_features)\n",
    "\n",
    "        if hog_dict is not None:\n",
    "            cspace = hog_dict['conv']\n",
    "            orient = hog_dict['orient']\n",
    "            pix_per_cell = hog_dict['pix_per_cell']\n",
    "            cell_per_block = hog_dict['cell_per_block']\n",
    "            channels = hog_dict['channels']       \n",
    "            trans_sqrt = hog_dict['trans_sqrt']\n",
    "            block_norm = hog_dict['block_norm']\n",
    "            image_conv = convert_color(image, cspace)  \n",
    "            hog_features = []\n",
    "            if channels == 'all':\n",
    "                channels = np.arange(3)\n",
    "            \n",
    "            for ch in channels:\n",
    "                hog_features.append(get_hog_features(image_conv[:,:,ch], \n",
    "                                    orient, pix_per_cell, cell_per_block, \n",
    "                                    vis=False, feature_vec=True, trans_sqrt=trans_sqrt, block_norm=block_norm))\n",
    "            hog_features = np.ravel(hog_features)                      \n",
    "            single_img_features.append(hog_features)\n",
    "            \n",
    "        features.append(np.concatenate(single_img_features))\n",
    "\n",
    "    # Return list of feature vectors\n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Save a dictionary into a pickle file\n",
    "def save_to_pickle(data, key_name, file_name):\n",
    "    if len(file_name) > 1:\n",
    "        for d, k, f in zip(data, key_name, file_name):\n",
    "            pickle_data = {k: d}\n",
    "            pickle.dump(pickle_data, open(f + '.p', \"wb\"))\n",
    "    else:\n",
    "        pickle_data = {}\n",
    "        for d, k in zip(data, key_name):\n",
    "            pickle_data[k] = d\n",
    "        print(pickle_data)\n",
    "        pickle.dump(pickle_data, open(file_name[0] + '.p', \"wb\"))\n",
    "    \n",
    "    return None\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def augment_data(image_files, filepath, rotate=False, angle=25, flip=[], adj_bright=False):\n",
    "    filler = '00000'\n",
    "    for ii, file in enumerate(image_files):\n",
    "        image = ndimage.imread(file)\n",
    "        suffix = filler[:len(filler) - len(str(ii + 1))] + str(ii + 1) + '.png'\n",
    "        if rotate:\n",
    "            filename = filepath + 'rotated_image_' + suffix\n",
    "            ang = random.uniform(-angle, angle)\n",
    "            rotated_img = ndimage.rotate(image, ang, reshape=False)\n",
    "            mpimg.imsave(filename, rotated_img)\n",
    "        for fi in flip:\n",
    "            assert fi in [0, 1, -1]\n",
    "            if fi == 0:\n",
    "                filename = filepath + 'horz_flip_image_' + suffix\n",
    "            elif fi == 1:\n",
    "                filename = filepath + 'vert_flip_image_' + suffix\n",
    "            elif fi == -1:\n",
    "                filename = filepath + 'both_flip_image_' + suffix\n",
    "            flipped_img = cv2.flip(image, fi)\n",
    "            mpimg.imsave(filename, flipped_img)\n",
    "        if adj_bright:\n",
    "            filename = filepath + 'bright_adj_image_' + suffix\n",
    "            random_bright = .25 + np.random.uniform()\n",
    "            random_bright = min(random_bright, 1.15)\n",
    "            HSV_image = cv2.cvtColor(image,cv2.COLOR_RGB2HSV)\n",
    "            HSV_image[:,:,2] = HSV_image[:,:,2] * random_bright\n",
    "            bright_adj_image = cv2.cvtColor(HSV_image, cv2.COLOR_HSV2RGB)\n",
    "            mpimg.imsave(filename, bright_adj_image)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_training_files(GTI=False, KITTI=False, udacity=False, augmented=False, rotated=None,\n",
    "                        flipped=[], bright_adj=None, len_non_cars=None, debug=False):\n",
    "    \n",
    "    gti_car_far = []\n",
    "    gti_car_left = []\n",
    "    gti_car_midclose = []\n",
    "    gti_car_right = []\n",
    "    gti_car_files = []\n",
    "    kitti_car_files = []\n",
    "    udacity_car_files = []\n",
    "    # Lists for augmented image files\n",
    "    aug_gti_car_files = []\n",
    "    aug_kitti_car_files = []\n",
    "    aug_udacity_car_files = []\n",
    "    aug_gti_non_car_files = []\n",
    "    aug_extras_non_car_files = []\n",
    "        \n",
    "    if augmented:\n",
    "        has_rotated = rotated is not None\n",
    "        flipped_is_list = (type(flipped) == list)\n",
    "        has_bright_adj = bright_adj is not None\n",
    "        if not(has_rotated & flipped_is_list & has_bright_adj):\n",
    "            raise Exception('If augmented is set to True, then you must provide True or False values for '\n",
    "                            'rotated and bright_adj, and you must provide a list with values of 0, 1, and/or -1 '\n",
    "                            'to flipped')\n",
    "        \n",
    "    if GTI:\n",
    "        # Collect the filenames for all car images in the dataset\n",
    "        gti_car_far = glob.glob('../data/vehicles/GTI_Far/*png')\n",
    "        gti_car_left = glob.glob('../data/vehicles/GTI_Left/*png')\n",
    "        gti_car_midclose = glob.glob('../data/vehicles/GTI_MiddleClose/*png')\n",
    "        gti_car_right = glob.glob('../data/vehicles/GTI_Right/*png')\n",
    "        gti_car_files = gti_car_far + gti_car_left + gti_car_midclose + gti_car_right\n",
    "        \n",
    "    if KITTI:\n",
    "        kitti_car_files = glob.glob('../data/vehicles/KITTI_extracted/*png')\n",
    "  \n",
    "    if udacity:\n",
    "        udacity_car_files = glob.glob('../data/udacity_cropped/*jpg')\n",
    "    \n",
    "    # Collect the filenames for all non-car images in the dataset\n",
    "    gti_non_car_files = glob.glob('../data/non_vehicles/GTI/*png')\n",
    "    extras_non_car_files = glob.glob('../data/non_vehicles/Extras/*png')\n",
    "    \n",
    "    # Generate augmented data if desired\n",
    "    if augmented:\n",
    "        # Check the augmentation operations the user selected\n",
    "        ops = [rotated, bright_adj, 0 in flipped, 1 in flipped, -1 in flipped]\n",
    "        # Assign the correct image suffix for corresponding augmentation op\n",
    "        suffixes = ['rotated', 'bright_adj', 'horz_flip', 'vert_flip', 'both_flip']\n",
    "        for suffix, op in zip(suffixes, ops):\n",
    "            aug_gti_car_far = []\n",
    "            aug_gti_car_left = []\n",
    "            aug_gti_car_midclose = []\n",
    "            aug_gti_car_right = []\n",
    "            if op:\n",
    "                # Collect image filenames for augmented car datasets\n",
    "                if 'GTI' in augmented:\n",
    "                    aug_gti_car_far = glob.glob('../data/augmented_images/vehicles/GTI_Far/' + suffix + '*png')\n",
    "                    aug_gti_car_left = glob.glob('../data/augmented_images/vehicles/GTI_Left/' + suffix + '*png')\n",
    "                    aug_gti_car_midclose = glob.glob('../data/augmented_images/vehicles/GTI_MiddleClose/' + \n",
    "                                                     suffix + '*png')\n",
    "                    aug_gti_car_right = glob.glob('../data/augmented_images/vehicles/GTI_Right/' + suffix + '*png')\n",
    "                    aug_gti_car_files += (aug_gti_car_far + aug_gti_car_left + aug_gti_car_midclose + \n",
    "                                          aug_gti_car_right)\n",
    "                if 'KITTI' in augmented:\n",
    "                    aug_kitti_car_files += glob.glob('../data/augmented_images/vehicles/KITTI_extracted/' + \n",
    "                                                     suffix + '*png')\n",
    "                if 'udacity' in augmented:\n",
    "                    aug_udacity_car_files += glob.glob('../data/augmented_images/vehicles/udacity_cropped/*png')\n",
    "                \n",
    "                if 'non-cars' in augmented:\n",
    "                    # Collect image filenames for augmented non-car datasets\n",
    "                    aug_gti_non_car_files += glob.glob('../data/augmented_images/non_vehicles/GTI/' + suffix + '*png')\n",
    "                    aug_extras_non_car_files += glob.glob('../data/augmented_images/non_vehicles/Extras/' + \n",
    "                                                          suffix + '*png')\n",
    "\n",
    "    ## Shuffle the data based on collection type (e.g. GTI, KITTI, etc.), subgroup (e.g. far, left, etc.), and\n",
    "    ## is applicable, augmented operation (e.g. rotated, flipped, etc.)\n",
    "    # For car dataset\n",
    "    if GTI:\n",
    "        gti_car_subgroups = ([1] * len(gti_car_far) + [2] * len(gti_car_left) + [3] * len(gti_car_midclose) +\n",
    "                             [4] * len(gti_car_right))\n",
    "        gti_car_1, gti_car_2, y1, y2 = train_test_split(gti_car_files, gti_car_subgroups, \n",
    "                                                        stratify=gti_car_subgroups, random_state = 42)\n",
    "        gti_car_files = gti_car_1 + gti_car_2\n",
    "    \n",
    "    # For the augmented car dataset, for each collection, shuffle the images within the collection\n",
    "    # based on the augmentation operation\n",
    "    if augmented:\n",
    "        ops = [rotated, bright_adj, 0 in flipped, 1 in flipped, -1 in flipped]\n",
    "        if 'GTI' in augmented:\n",
    "            gti_aug_op_labels = []\n",
    "            collection_lengths = [len(gti_car_far), len(gti_car_left), len(gti_car_midclose), len(gti_car_right)]\n",
    "            ii = 1\n",
    "            for _ in range(sum(ops)):\n",
    "                for length in collection_lengths:\n",
    "                    gti_aug_op_labels += [ii] * length\n",
    "                    ii += 1\n",
    "            aug_gti_car_x, aug_gti_car_v, _, _ = train_test_split(aug_gti_car_files, gti_aug_op_labels,\n",
    "                                                                      stratify=gti_aug_op_labels, random_state=42)\n",
    "            aug_gti_car_files = aug_gti_car_x + aug_gti_car_v\n",
    "        \n",
    "        if 'KITTI' in augmented:\n",
    "            ii = 1\n",
    "            kitti_aug_op_labels = []\n",
    "            for _ in range(sum(ops)):\n",
    "                kitti_aug_op_labels += [ii] * len(kitti_car_files)\n",
    "                ii += 1\n",
    "            aug_kitti_car_x, aug_kitti_car_v, y_t, y_v = train_test_split(aug_kitti_car_files, kitti_aug_op_labels,\n",
    "                                                                          stratify=kitti_aug_op_labels, \n",
    "                                                                          random_state=42)\n",
    "            aug_kitti_car_files = aug_kitti_car_x + aug_kitti_car_v\n",
    "        \n",
    "        if 'udacity' in augmented:\n",
    "            ii = 1\n",
    "            udacity_aug_op_labels = []\n",
    "            for _ in range(sum(ops)):\n",
    "                udacity_aug_op_labels += [ii] * len(udacity_car_files)\n",
    "                ii += 1\n",
    "            aug_udacity_car_x, aug_udacity_car_v, _, _ = train_test_split(aug_udacity_car_files, \n",
    "                                                                              udacity_aug_op_labels,\n",
    "                                                                              stratify=udacity_aug_op_labels, \n",
    "                                                                              random_state=42)\n",
    "            aug_udacity_car_files = aug_udacity_car_x + aug_udacity_car_v\n",
    "        \n",
    "    # Combine all the car and augmented car files and shuffle by collection(e.g. GTI, KITTI, etc.)\n",
    "    car_files = (gti_car_files + aug_gti_car_files + kitti_car_files + aug_kitti_car_files +\n",
    "                udacity_car_files + aug_udacity_car_files)\n",
    "    car_labels = ([1] * len(gti_car_files) + [2] * len(aug_gti_car_files) + [3] * len(kitti_car_files) +\n",
    "                  [4] * len(aug_kitti_car_files) + [5] * len(udacity_car_files) + \n",
    "                  [6] * len(aug_udacity_car_files))\n",
    "    car_files_x, car_files_v, _, _ = train_test_split(car_files, car_labels, stratify=car_labels,\n",
    "                                                      random_state=42)\n",
    "    car_files = car_files_x + car_files_v\n",
    "        \n",
    "    # Now to shuffle the non-car datasets\n",
    "    non_car_labels = [1] * len(gti_non_car_files) + [2] * len(extras_non_car_files)\n",
    "    if augmented:\n",
    "        if 'non-cars' in augmented:\n",
    "            ii = 3\n",
    "            collection_lengths = [len(gti_non_car_files), len(extras_non_car_files)]\n",
    "            for length in collection_lengths:\n",
    "                for _ in range(sum(ops)):\n",
    "                    non_car_labels += [ii] * length\n",
    "                    ii += 1\n",
    "    \n",
    "    non_car_files = gti_non_car_files + extras_non_car_files + aug_gti_non_car_files + aug_extras_non_car_files\n",
    "    non_car_files_x, non_car_files_v, y_t, y_v = train_test_split(non_car_files, non_car_labels,\n",
    "                                                              stratify=non_car_labels, random_state=42)\n",
    "    non_car_files = non_car_files_x + non_car_files_v\n",
    "    if len_non_cars is not None:\n",
    "        if len_non_cars > 1:\n",
    "            for _ in range(len_non_cars - 1):\n",
    "                non_car_files += shuffle(non_car_files)\n",
    "        else:\n",
    "            non_car_files = non_car_files[:int(len_non_cars * len(non_car_files))]\n",
    "           \n",
    "    all_img_files = np.hstack((np.array(non_car_files), np.array(car_files)))\n",
    "    img_type_labels = np.hstack((np.zeros(len(non_car_files)), np.ones(len(car_files))))\n",
    "    \n",
    "    print('No. of GTI car files: {}'.format(len(gti_car_files)))\n",
    "    print('No. of KITTI car files: {}'.format(len(kitti_car_files)))\n",
    "    print('No. of Udacity car files: {}'.format(len(udacity_car_files)))\n",
    "    \n",
    "    print('\\nNo. of Augmented GTI car files: {}'.format(len(aug_gti_car_files)))\n",
    "    print('No. of Augmented KITTI car files: {}'.format(len(aug_kitti_car_files)))\n",
    "    print('No. of Augmented Udacity car files: {}'.format(len(aug_udacity_car_files)))\n",
    "    \n",
    "    print('\\nNo. of GTI non-car files: {}'.format(len(gti_non_car_files)))\n",
    "    print('No. of Extras non-car files: {}'.format(len(extras_non_car_files)))\n",
    "    \n",
    "    print('\\nNo. of Augmented GTI non-car files: {}'.format(len(aug_gti_non_car_files)))\n",
    "    print('No. of Augmented Extras non-car files: {}'.format(len(aug_extras_non_car_files)))\n",
    "    \n",
    "    print('Total number of car files: {}'.format(len(car_files)))\n",
    "    print('Total number of non-car files: {}'.format(len(non_car_files)))\n",
    "    print('Total number of images: {}'.format(all_img_files.shape))\n",
    "    print('Total number of labels: {}'.format(img_type_labels.shape))\n",
    "\n",
    "    train_img_files = np.copy(all_img_files)\n",
    "    train_labels_copy = np.copy(img_type_labels)\n",
    "    \n",
    "    if debug:\n",
    "        print('\\nNo. of GTI Far files: {}'.format(len(gti_car_far)), \n",
    "              '\\nNo. of GTI Left files: {}'.format(len(gti_car_left)),\n",
    "              '\\nNo. of GTI MidClose files: {}'.format(len(gti_car_midclose)),\n",
    "              '\\nNo. of GTI Right files: {}'.format(len(gti_car_right)))\n",
    "        \n",
    "        return ((aug_gti_car_files, aug_kitti_car_files, aug_gti_non_car_files, aug_extras_non_car_files), \n",
    "                train_img_files, train_labels_copy)\n",
    "    else:\n",
    "        return train_img_files, train_labels_copy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def make_svc(file_end, settings, dicts):\n",
    "    GTI = settings['GTI']\n",
    "    KITTI = settings['KITTI']\n",
    "    udacity = settings['udacity']\n",
    "    augmented = settings['aug']\n",
    "    rotated = settings['rotated']\n",
    "    flipped = settings['flipped']\n",
    "    bright_adj = settings['bright_adj']\n",
    "    len_non_cars = settings['len_non_cars']\n",
    "    \n",
    "    train_img_files, train_labels_copy = make_training_files(GTI=GTI, KITTI=KITTI, udacity=udacity, \n",
    "                                                             augmented=augmented,\n",
    "                                                             rotated=rotated, flipped=flipped, bright_adj=bright_adj, \n",
    "                                                             len_non_cars=len_non_cars)\n",
    "    \n",
    "    spatial_dict = dicts['spatial']\n",
    "    hist_dict = dicts['hist']\n",
    "    hog_dict = dicts['hog']\n",
    "    X_train_features = extract_features(train_img_files, spatial_dict=spatial_dict, hist_dict=hist_dict, \n",
    "                                        hog_dict=hog_dict)\n",
    "    X_train_features = np.array(X_train_features).astype(np.float32)\n",
    "    # Fit a per-column scaler\n",
    "    X_scaler = StandardScaler().fit(X_train_features)\n",
    "    # Apply the scaler to X\n",
    "    scaled_X = X_scaler.transform(X_train_features)\n",
    "    print('X train features shape is {}'.format(X_train_features.shape))\n",
    "    print('Scaled X features is {}'.format(scaled_X.shape))\n",
    "\n",
    "    save_to_pickle([scaled_X, train_labels_copy, X_scaler], ['features', 'labels', 'scaler'], \n",
    "                   ['../pickle/scaled_X_' + file_end])\n",
    "    save_to_pickle([spatial_dict, hist_dict, hog_dict], ['spatial', 'hist', 'hog'], \n",
    "                   ['../pickle/dict_' + file_end])\n",
    "\n",
    "    rand_state = np.random.randint(0, 100)\n",
    "    # Shuffle and split the data into a training and test set\n",
    "    X_train, X_validate, y_train, y_validate = train_test_split(scaled_X, train_labels_copy, test_size=0.2, \n",
    "                                                                stratify=train_labels_copy, random_state=rand_state)\n",
    "    print('X_train shape is {}'.format(X_train.shape))\n",
    "    print('y_train shape is {}'.format(y_train.shape))\n",
    "    print('X_validate shape is {}'.format(X_validate.shape))\n",
    "    print('y_validate shape is {}'.format(y_validate.shape))\n",
    "    print(y_train[:20])\n",
    "\n",
    "    # Use a linear SVC \n",
    "    svc = LinearSVC()\n",
    "    # Check the training time for the SVC\n",
    "    t=time.time()\n",
    "    svc.fit(X_train, y_train)\n",
    "    t2 = time.time()\n",
    "    print(round(t2-t, 2), 'Seconds to train SVC...')\n",
    "    print('Test Accuracy of SVC = ', round(svc.score(X_validate, y_validate), 4))\n",
    "    save_to_pickle([svc, hog_dict['trans_sqrt'], hog_dict['block_norm']], ['svc', 'trans_sqrt', 'block_norm'], \n",
    "                   ['../pickle/svc_pickle_' + file_end])\n",
    "\n",
    "def load_data(file_end):\n",
    "    dict_pickle = pickle.load( open(\"../pickle/scaled_X_\" + file_end + \".p\", \"rb\") )\n",
    "    X_scaler = dict_pickle['scaler']\n",
    "\n",
    "    dict_pickle = pickle.load( open(\"../pickle/dict_\" + file_end + \".p\", \"rb\") )\n",
    "    spatial_dict = dict_pickle['spatial']\n",
    "    hist_dict = dict_pickle['hist']\n",
    "    hog_dict = dict_pickle['hog']\n",
    "\n",
    "    dict_pickle = pickle.load( open(\"../pickle/svc_pickle_\" + file_end + \".p\", \"rb\" ) )\n",
    "    svc = dict_pickle['svc']\n",
    "    \n",
    "    return X_scaler, spatial_dict, hist_dict, hog_dict, svc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define a single function that can extract features using hog sub-sampling \n",
    "# and make predictions\n",
    "def find_cars(img, img_format, xstart, xstop, ystart, ystop, scale, X_scaler, svc, hog_dict, \n",
    "              spatial_dict=None, hist_dict=None, print_pred=False):\n",
    "                \n",
    "    # Extract the Hog parameters\n",
    "    hg_cspace = hog_dict['conv']\n",
    "    orient = hog_dict['orient']\n",
    "    pix_per_cell = hog_dict['pix_per_cell']\n",
    "    cell_per_block = hog_dict['cell_per_block']\n",
    "    hg_ch = hog_dict['channels']       \n",
    "    trans_sqrt = hog_dict['trans_sqrt']\n",
    "    block_norm = hog_dict['block_norm']\n",
    "    \n",
    "    if hg_ch == 'all':\n",
    "        hg_ch = np.arange(3)\n",
    "    \n",
    "    draw_img = np.copy(img)\n",
    "    if img_format == 'jpg':\n",
    "        img = img.astype(np.float32) / 255\n",
    "     \n",
    "    # Compute individual channel HOG features for the entire image\n",
    "    img_tosearch = img[ystart:ystop, xstart:xstop, :]\n",
    "    ctrans_tosearch = convert_color(img_tosearch, conv=hg_cspace)\n",
    "    bboxes = [] # Store the coordinates of the bounded boxes\n",
    "\n",
    "    if scale != 1:\n",
    "        imshape = ctrans_tosearch.shape\n",
    "        ctrans_tosearch = cv2.resize(ctrans_tosearch, (np.int(imshape[1] / scale), \n",
    "                                     np.int(imshape[0] / scale)))\n",
    "    chs = []\n",
    "    for ch in hg_ch:\n",
    "        chs.append(ctrans_tosearch[:,:,ch])\n",
    "\n",
    "    # Define blocks and steps\n",
    "    nxblocks = (chs[0].shape[1] // pix_per_cell) - cell_per_block + 1\n",
    "    nyblocks = (chs[0].shape[0] // pix_per_cell) - cell_per_block + 1 \n",
    "    nfeat_per_block = orient*cell_per_block**2\n",
    "\n",
    "    # 64 was the orginal sampling rate, with 8 cells and 8 pix per cell\n",
    "    window = 64\n",
    "    nblocks_per_window = (window // pix_per_cell) - cell_per_block + 1\n",
    "    cells_per_step = 2  # Instead of overlap, define how many cells to step\n",
    "    nxsteps = (nxblocks - nblocks_per_window) // cells_per_step + 1\n",
    "    nysteps = (nyblocks - nblocks_per_window) // cells_per_step + 1\n",
    "\n",
    "    hogs = []\n",
    "    for ch in chs:\n",
    "        hog_feature = get_hog_features(ch, orient, pix_per_cell, cell_per_block, \n",
    "                                feature_vec=False, trans_sqrt=trans_sqrt, block_norm=block_norm)\n",
    "        hogs.append(hog_feature)\n",
    "    \n",
    "    for xb in range(nxsteps):\n",
    "        for yb in range(nysteps):\n",
    "            ypos = yb*cells_per_step\n",
    "            xpos = xb*cells_per_step\n",
    "\n",
    "            # Extract HOG for this patch\n",
    "            hog_list = []\n",
    "            for hg in hogs:    \n",
    "                hog_vect = hg[ypos:ypos+nblocks_per_window, \n",
    "                              xpos:xpos+nblocks_per_window].ravel()\n",
    "                hog_list.append(hog_vect)   \n",
    "            hog_features = np.hstack(hog_list)\n",
    "        \n",
    "            xleft = xpos*pix_per_cell\n",
    "            ytop = ypos*pix_per_cell\n",
    "\n",
    "           # Extract the image patch\n",
    "            subimg = cv2.resize(ctrans_tosearch[ytop:ytop+window, \n",
    "                                xleft:xleft+window], (64,64))\n",
    "\n",
    "            # Get color features\n",
    "            spatial_features = np.array([])\n",
    "            hist_features = np.array([])\n",
    "            if spatial_dict is not None:\n",
    "                sp_cspace = spatial_dict['conv']\n",
    "                spatial_size = spatial_dict['size']\n",
    "                sp_ch = spatial_dict['channels']\n",
    "                spatial_features = bin_spatial(subimg, size=(spatial_size, spatial_size), channel=sp_ch)\n",
    "            if hist_dict is not None:\n",
    "                hs_cspace = hist_dict['conv']\n",
    "                hist_bins = hist_dict['nbins']\n",
    "                bin_range = hist_dict['bin_range']\n",
    "                hi_ch = hist_dict['channels']\n",
    "                hist_features = color_hist(subimg, nbins=hist_bins, bins_range=(0, 256), channel=hi_ch)\n",
    "\n",
    "            # Scale features and make a prediction\n",
    "            features = np.hstack((spatial_features, hist_features, hog_features))\n",
    "            test_features = X_scaler.transform(features.reshape(1, -1))       \n",
    "            test_prediction = svc.predict(test_features)\n",
    "            if print_pred:\n",
    "                print('prediction: {}'.format(test_prediction))\n",
    "            if test_prediction == 1:\n",
    "                xbox_left = np.int(xleft*scale)\n",
    "                ytop_draw = np.int(ytop*scale)\n",
    "                win_draw = np.int(window*scale)\n",
    "                cv2.rectangle(draw_img,(xbox_left+xstart, ytop_draw+ystart),\n",
    "                (xbox_left+xstart+win_draw,ytop_draw+win_draw+ystart),(0,0,255),6) \n",
    "                bboxes.append(((xbox_left + xstart, ytop_draw + ystart), \n",
    "                               (xbox_left + xstart + win_draw, ytop_draw + win_draw + ystart)))\n",
    "                \n",
    "    return draw_img, bboxes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_heat(heatmap, bbox_list):\n",
    "    # Iterate through list of bboxes\n",
    "    for box in bbox_list:\n",
    "        # Add += 1 for all pixels inside each bbox\n",
    "        # Assuming each \"box\" takes the form ((x1, y1), (x2, y2))\n",
    "        heatmap[box[0][1]:box[1][1], box[0][0]:box[1][0]] += 1\n",
    "\n",
    "    # Return updated heatmap\n",
    "    return heatmap\n",
    "\n",
    "def apply_threshold(heatmap, threshold):\n",
    "    # Zero out pixels below the threshold\n",
    "    output_heatmap = np.copy(heatmap)\n",
    "    output_heatmap[output_heatmap < threshold] = 0\n",
    "    \n",
    "    # Return thresholded map\n",
    "    return output_heatmap\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_labeled_bboxes(image, heat, threshold, draw, color=(0, 0, 255), thickness=3):\n",
    "    out_bboxes = {} # A dict of all the bounded boxes for each car found in this image\n",
    "    \n",
    "    # Accumulate bounded boxes across several frames and then apply heat-map with threshold\n",
    "    heat_map = apply_threshold(heat, threshold)\n",
    "    labels = label(heat_map)\n",
    "\n",
    "    # Iterate through all detected cars\n",
    "    for car_number in range(1, labels[1]+1):\n",
    "        # Find pixels with each car_number label value\n",
    "        nonzero = (labels[0] == car_number).nonzero()\n",
    "        # Identify x and y values of those pixels\n",
    "        nonzeroy = np.array(nonzero[0])\n",
    "        nonzerox = np.array(nonzero[1])\n",
    "        # Define a bounding box based on min/max x and y\n",
    "        bbox = ((np.min(nonzerox), np.min(nonzeroy)), (np.max(nonzerox), np.max(nonzeroy)))\n",
    "        out_bboxes[car_number] = bbox\n",
    "        if draw:\n",
    "            # Draw the box on the image\n",
    "            cv2.rectangle(image, bbox[0], bbox[1], color, thickness)\n",
    "    \n",
    "    if draw:\n",
    "        # Return the image and bboxes\n",
    "        return image, out_bboxes\n",
    "    else:\n",
    "        # Return the bboxes\n",
    "        return out_bboxes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from moviepy.editor import VideoFileClip\n",
    "from IPython.display import HTML\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define a class to keep track of data among frames\n",
    "class Frame():\n",
    "    def __init__(self, img_shape):\n",
    "        self.image = None\n",
    "        self.frame_count = 0\n",
    "        self.car_accum = False\n",
    "        self.state = 'detect'\n",
    "        self.show_bboxes = False\n",
    "        self.debug = False\n",
    "        self.detect_count = 0 # Tracks the number of frames that have passed\n",
    "        self.detect_samples = 10 # How many frames should we collect boxes for\n",
    "        self.detect_thresh = 9\n",
    "        self.accum_thresh = 2\n",
    "        self.track_count = 0\n",
    "        self.track_samples = 2        \n",
    "        self.track_thresh = 2\n",
    "        self.track_length = 1\n",
    "        self.low_conf_detections = {}\n",
    "        self.low_conf_thresh = 2\n",
    "        self.overlap_ratio = {}\n",
    "        self.ratio_text = 'Not enough overlap in cars: '\n",
    "        self.detect_heat = np.zeros(img_shape)\n",
    "        self.track_heat = np.zeros(img_shape)\n",
    "        self.accum_heat = np.zeros(img_shape)\n",
    "        self.detect_heat_record = []\n",
    "        self.track_heat_record = []\n",
    "        self.accum_heat_record = []\n",
    "        self.car_boxes = defaultdict(list)\n",
    "        self.final_boxes = defaultdict(list)\n",
    "\n",
    "    def show_bbox_details(self, thickness, colors, box_lists):\n",
    "        for t, color, box_list in zip(thickness, colors, box_lists):\n",
    "            for box in box_list:\n",
    "                cv2.rectangle(self.image, box[0], box[1], color, t)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This is the function that processes each frame of the video to find cars and apply a box around them\n",
    "def process_image(image):\n",
    "    \n",
    "    frame.image = np.copy(image)\n",
    "    track_boxes = []\n",
    "    test_boxes = []\n",
    "    detect_boxes = []\n",
    "    accum_detect_boxes = []\n",
    "    search_box = []\n",
    "    car_boxes = []    \n",
    "    reset_text = ''\n",
    "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "    if frame.debug:\n",
    "        cv2.putText(frame.image, 'start state: {}'.format(frame.state), (650, 50), font, 1, \n",
    "                    (200,255,155), 2, cv2.LINE_AA) \n",
    "        cv2.putText(frame.image, 'frame count: {}'.format(frame.frame_count), (650, 150), font, 1, \n",
    "                    (200,255,155), 2, cv2.LINE_AA)  \n",
    "        \n",
    "    if frame.state == 'detect':     \n",
    "        # Search for cars in the distance\n",
    "        xstart = 0\n",
    "        xstop = image.shape[1]\n",
    "        ycoords = [(400, 550), (400, 6000), (400, 656)]\n",
    "#         scales = [1, 1.5, 1.7, 2]\n",
    "#         scales = [0.8, 1, 1.5]\n",
    "#         scales = [0.5, 1, 1.5]\n",
    "        scales = [1, 1.25, 1.5]\n",
    "        if len(ycoords) != len(scales):\n",
    "            raise Exception('The number of image detection windows must equal the number of detection scales')\n",
    "        for ycoord, scale in zip(ycoords, scales):\n",
    "            ystart, ystop = ycoord[0], ycoord[1]\n",
    "            _, bboxes = find_cars(image, img_format, xstart, xstop, ystart, ystop, scale, X_scaler,\n",
    "                                  svc, hog_dict, spatial_dict, hist_dict)\n",
    "            detect_boxes += bboxes\n",
    "        \n",
    "        if frame.car_accum:\n",
    "            # Accumulate bounded boxes across several frames and then apply heat-map with threshold\n",
    "            heat = np.zeros_like(frame.image[:, :, 0])\n",
    "            heat = add_heat(heat, detect_boxes)\n",
    "            car_boxes = make_labeled_bboxes(frame.image, heat, threshold=frame.accum_thresh, draw=False)\n",
    "            for car, box in car_boxes.items():\n",
    "                accum_detect_boxes += [box]\n",
    "            frame.accum_heat = add_heat(frame.accum_heat, accum_detect_boxes)\n",
    "        else:\n",
    "            frame.detect_heat = add_heat(frame.detect_heat, detect_boxes)\n",
    "        \n",
    "        # Store a copy of the current heat map for debugging\n",
    "        # Note that the index of the current heat map being added to the list equals the current detect_count\n",
    "        frame.detect_heat_record.append(np.copy(frame.detect_heat))\n",
    "        frame.accum_heat_record.append(np.copy(frame.accum_heat))\n",
    "\n",
    "        if frame.debug:\n",
    "            cv2.putText(frame.image, 'detect count: ' + str(frame.detect_count), (650, 200), \n",
    "                        font, 1, (200,255,155), 2, cv2.LINE_AA)\n",
    "        \n",
    "        frame.detect_count += 1\n",
    "        if (frame.detect_count % frame.detect_samples) == 0:\n",
    "            if frame.car_accum:\n",
    "                frame.final_boxes = make_labeled_bboxes(frame.image, frame.accum_heat,\n",
    "                                                        threshold=frame.detect_thresh, draw=False)            \n",
    "            else:\n",
    "                frame.final_boxes = make_labeled_bboxes(frame.image, frame.detect_heat,\n",
    "                                                        threshold=frame.detect_thresh, draw=False)\n",
    "            # Reset the store bboxes\n",
    "            frame.detect_heat = np.zeros_like(frame.image[:, :, 0])\n",
    "            frame.accum_heat = np.zeros_like(frame.image[:, :, 0])\n",
    "            if len(frame.final_boxes) > 0:\n",
    "                frame.state = 'track'  \n",
    "                reset_text = 'go track'\n",
    "            else:\n",
    "                frame.state = 'detect'\n",
    "                reset_text = 'reset'\n",
    "                    \n",
    "    if frame.state == 'track':              \n",
    "        adj_x_l, adj_x_r = 15, 15\n",
    "        adj_y_b, adj_y_t = 10, 5\n",
    "        for car, box in frame.final_boxes.items():            \n",
    "            ystart = np.clip(box[0][1] - adj_y_t, 0, frame.image.shape[0])\n",
    "            ystop = np.clip(box[1][1] + adj_y_b, 0, frame.image.shape[0])\n",
    "            xstart = np.clip(box[0][0] - adj_x_l, 0, frame.image.shape[1])\n",
    "            xstop = np.clip(box[1][0] + adj_x_r, 0, frame.image.shape[1])      \n",
    "            search_box += [((xstart, ystart), (xstop, ystop))]\n",
    "            single_car_boxes= []\n",
    "            scales = [0.5, 0.8, 1.1]\n",
    "            for scale in scales:\n",
    "                _, bboxes = find_cars(image, img_format, xstart, xstop, ystart, ystop, scale, X_scaler,\n",
    "                                      svc, hog_dict, spatial_dict, hist_dict)\n",
    "                single_car_boxes += bboxes\n",
    "            \n",
    "            track_boxes += single_car_boxes\n",
    "            frame.car_boxes[car] += single_car_boxes\n",
    "            frame.track_heat = add_heat(frame.track_heat, single_car_boxes)\n",
    "        \n",
    "        # Store a copy of the current heat map for debugging\n",
    "        # Note that the index of the current heat map being added to the list equals the current track_count\n",
    "        frame.track_heat_record.append(np.copy(frame.track_heat)) \n",
    "        if frame.debug:\n",
    "            cv2.putText(frame.image, 'track count: ' + str(frame.track_count), (650, 250), \n",
    "                        font, 1, (200,255,155), 2, cv2.LINE_AA)\n",
    "\n",
    "        frame.track_count += 1 \n",
    "        if (frame.track_count % frame.track_samples) == 0:\n",
    "            reset_text = 'go verify'  \n",
    "            frame.state = 'verify'\n",
    "            frame.track_heat = np.zeros_like(frame.image[:, :, 0])\n",
    "\n",
    "    if frame.state == 'verify':\n",
    "        false_detections = [] # Detections that are false positives\n",
    "        too_low_conf = False\n",
    "        for car, boxes in frame.car_boxes.items():\n",
    "            heat = np.zeros_like(frame.image[:, :, 0])\n",
    "            heat = add_heat(heat, boxes)\n",
    "            test_car_boxes = make_labeled_bboxes(frame.image, heat, \n",
    "                                                 threshold=frame.track_thresh, draw=False)   \n",
    "            if test_car_boxes:\n",
    "                org_box = frame.final_boxes[car]\n",
    "                left_x, top_y = [], []\n",
    "                right_x, bottom_y = [], []\n",
    "                for _, box in test_car_boxes.items():\n",
    "                    left_x += [box[0][0]]\n",
    "                    top_y += [box[0][1]]\n",
    "                    right_x += [box[1][0]]\n",
    "                    bottom_y += [box[1][1]]\n",
    "  \n",
    "                test_box = ((min(left_x), min(top_y)), (max(right_x), max(bottom_y)))\n",
    "                test_boxes += [test_box]\n",
    "                # For there to be an intersection of boxes, min R edge > max L edge and \n",
    "                # min bot edge > max top edge\n",
    "                min_right_edge = min(test_box[1][0], org_box[1][0])\n",
    "                max_left_edge = max(test_box[0][0], org_box[0][0])\n",
    "                min_bottom_edge = min(test_box[1][1], org_box[1][1])\n",
    "                max_top_edge = max(test_box[0][1], org_box[0][1])\n",
    "                overlap = ((max_left_edge, max_top_edge), (min_right_edge, min_bottom_edge))\n",
    "                overlap_area = (min_right_edge - max_left_edge) * (min_bottom_edge - max_top_edge)\n",
    "                org_area = (org_box[1][0] - org_box[0][0]) * (org_box[1][1] - org_box[0][1])\n",
    "                overlap_ratio = overlap_area / np.float(org_area)\n",
    "                frame.overlap_ratio[car] = overlap_ratio\n",
    "                if ((min_right_edge > max_left_edge) & (min_bottom_edge > max_top_edge) & \n",
    "                    (overlap_ratio >= 0.5)):   \n",
    "                    frame.final_boxes[car] = test_box\n",
    "                    if car in frame.low_conf_detections:\n",
    "                        frame.low_conf_detections[car] = 0\n",
    "                else:\n",
    "                    frame.low_conf_detections[car] = frame.low_conf_detections.get(car, 0) + 1               \n",
    "            else:\n",
    "                false_detections += [car]\n",
    "                \n",
    "        frame.car_boxes = defaultdict(list) \n",
    "        if frame.low_conf_detections:\n",
    "            text = 'Not enough overlap in cars: '\n",
    "            for ii, car in enumerate(frame.low_conf_detections):\n",
    "                if frame.low_conf_detections[car] > 0:\n",
    "                    text += (str(car) + '->' + '%.2f' % frame.overlap_ratio[car])\n",
    "                    if ii < (len(frame.low_conf_detections) - 1):\n",
    "                        text += ', '     \n",
    "                    \n",
    "                if frame.low_conf_detections[car] >= frame.low_conf_thresh:\n",
    "                    too_low_conf = True\n",
    "                    break\n",
    "\n",
    "            if frame.debug:\n",
    "                cv2.putText(frame.image, text, (650, 300), \n",
    "                            font, 1, (200,255,155), 2, cv2.LINE_AA)     \n",
    "                \n",
    "        if false_detections:\n",
    "            text = 'False positives in detections: '\n",
    "            for ii, car in enumerate(false_detections):\n",
    "                frame.final_boxes.pop(car, None)\n",
    "                text += str(car)\n",
    "                if ii < (len(false_detections) - 1):\n",
    "                    text += ', '  \n",
    "                    \n",
    "            if frame.debug:\n",
    "                cv2.putText(frame.image, text, (650, 350), font, 1, \n",
    "                            (200,255,155), 2, cv2.LINE_AA)\n",
    "        \n",
    "        if ((len(frame.final_boxes) == 0) | ((frame.track_count % frame.track_length) == 0) | too_low_conf):\n",
    "            reset_text = 'back to detect'\n",
    "            frame.state = 'detect'\n",
    "            frame.low_conf_detections = {}\n",
    "        else:\n",
    "            frame.state = 'track'\n",
    "    \n",
    "    for car, box in frame.final_boxes.items():\n",
    "        cv2.rectangle(frame.image, box[0], box[1], (0, 0, 255), 6)  \n",
    "        \n",
    "    if frame.show_bboxes:\n",
    "        if frame.car_accum:\n",
    "            frame.show_bbox_details(thickness=[3, 2, 4, 3, 2], \n",
    "                                    colors=[(255, 255, 0), (100, 100, 0), (0, 0, 0), (0, 255, 255), \n",
    "                                            (100, 0, 100)],\n",
    "                                    box_lists= [accum_detect_boxes, detect_boxes, search_box, \n",
    "                                                test_boxes, track_boxes])\n",
    "        else:\n",
    "            frame.show_bbox_details(thickness=[2, 4, 3, 2], \n",
    "                                    colors=[(100, 100, 0), (0, 0, 0), (0, 255, 255), (100, 0, 100)],\n",
    "                                    box_lists= [detect_boxes, search_box, test_boxes, track_boxes])\n",
    "    \n",
    "    if frame.debug:\n",
    "        cv2.putText(frame.image, 'end state: {}'.format(frame.state), (650, 100), font, 1, \n",
    "                    (200,255,155), 2, cv2.LINE_AA) \n",
    "        cv2.putText(frame.image, reset_text, (1000, 50), \n",
    "                    font, 1, (255, 0, 0), 2, cv2.LINE_AA)  \n",
    "        \n",
    "    frame.frame_count += 1  \n",
    "    \n",
    "    return frame.image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'trans_sqrt': True, 'block_norm': 'L1', 'conv': 'RGB2YCrCb', 'channels': 'all', 'cell_per_block': 2, 'orient': 9, 'pix_per_cell': 8}\n"
     ]
    }
   ],
   "source": [
    "need_svc = False\n",
    "file_end = 'ycrcb'\n",
    "settings = {'GTI': True, 'KITTI': True, 'udacity': False, 'aug': [], \n",
    "            'rotated': False, 'flipped':[], \n",
    "            'bright_adj': False, 'len_non_cars': None}\n",
    "\n",
    "## Here is a list of possible color spaces to use\n",
    "'''cv2.COLOR_RGB2HSV\n",
    "   cv2.COLOR_RGB2LUV\n",
    "   cv2.COLOR_RGB2HLS\n",
    "   cv2.COLOR_RGB2YUV\n",
    "   cv2.COLOR_RGB2YCrCb'''\n",
    "    \n",
    "spatial_dict = {'conv': 'RGB2YCrCb', 'size': 32, 'channels': 'all'}\n",
    "hist_dict = {'conv': 'RGB2YCrCb', 'nbins': 32, 'bin_range': (0, 256), 'channels': 'all'}\n",
    "hog_dict = {'conv': 'RGB2HSV', 'orient': 9, 'pix_per_cell': 8, 'cell_per_block': 2, 'channels': 'all',\n",
    "            'trans_sqrt': True, 'block_norm': 'L1'}\n",
    "dicts = {'spatial': spatial_dict, 'hist': hist_dict, 'hog': hog_dict}\n",
    "if need_svc:\n",
    "    make_svc(file_end, settings, dicts)\n",
    "X_scaler, spatial_dict, hist_dict, hog_dict, svc = load_data(file_end)\n",
    "print(hog_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|        | 148/1261 [04:50<37:17,  2.01s/it]"
     ]
    }
   ],
   "source": [
    "img_format = 'jpg'\n",
    "img = mpimg.imread('../test_images/test1.jpg')\n",
    "frame = Frame(img.shape[:2])\n",
    "frame.show_bboxes = False\n",
    "frame.debug = False\n",
    "frame.car_accum = True\n",
    "if frame.car_accum:\n",
    "    frame.accum_thresh = 1\n",
    "frame.detect_samples = 3\n",
    "frame.detect_thresh = 3\n",
    "frame.track_samples = 3       \n",
    "frame.track_thresh = 3\n",
    "frame.track_length = 6\n",
    "\n",
    "if (frame.track_length % frame.track_samples) != 0:\n",
    "    raise Exception('Please make track_length a multiple of track_samples')\n",
    "    \n",
    "video_name = 'project'\n",
    "output_name = 'debug_' + video_name\n",
    "ii = 10\n",
    "clip1 = VideoFileClip(\"../\" + video_name + \"_video.mp4\", audio=False)\n",
    "road_clip = clip1.fl_image(process_image)\n",
    "%time road_clip.write_videofile(\"../\" + output_name + \"_output_\" + str(ii) + \".mp4\", audio=False, verbose=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import PyQt5\n",
    "%matplotlib qt5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_heat_maps(heat_record, rows, cols, dev, title):\n",
    "    fig, axes = plt.subplots(rows, cols)\n",
    "    plt.tight_layout()\n",
    "    plt.suptitle(title, fontsize=10)\n",
    "    for i, ax in enumerate(axes.ravel()):\n",
    "        ax.imshow(heat_record[i + dev], cmap='hot')\n",
    "        ax.set_title('Frame {}'.format(i + dev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# plot_heat_maps(frame.accum_heat_record, 5, 2, 631, 'Accumulated')\n",
    "plot_heat_maps(frame.track_heat_record, 5, 2, 0, 'track')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
